\documentclass{report}

\usepackage{epsf}

\author{Walter Daelemans \and Jakub Zavrel\and Ko van der Sloot \and
	Antal van den Bosch\\ \ \\
	Induction of Linguistic Knowledge\\
	Computational Linguistics\\ 
        Tilburg University \\ 
        P.O. Box 90153, NL-5000 LE, Tilburg, The Netherlands \\ 
        URL: http://ilk.kub.nl\thanks{This document is available from
	http://ilk.kub.nl/\~{}ilk/papers/ilk9901.ps.gz. All rights reserved
	Induction of Linguistic Knowledge, Tilburg University.}}

\title{{\huge TiMBL: Tilburg Memory Based Learner}\\version
2.0\\{\huge Reference Guide}\\ \ \\ILK Technical Report -- ILK 99-01}

\begin{document}

\pagenumbering{roman} 

\maketitle

\tableofcontents

\chapter*{Preface}

Memory-Based Learning ({\sc mbl}) has proven to be successful in
a large number of tasks in Natural Language Processing (NLP). In our
group at Tilburg University we have been working since the end of the
1980's on the development of Memory-Based Learning techniques and
algorithms\footnote{Section~\ref{furtherreading} provides a historical
overview of our work on the application of {\sc mbl} in {\sc
nlp}.}. With the establishment of the ILK (Induction of Linguistic
Knowledge) research group in 1997, the need for a well-coded and
uniform tool for our main algorithms became more urgent. TiMBL is the
result of combining ideas from a number of different {\sc mbl}
implementations, cleaning up the interface, and using a whole bag of
tricks to make it more efficient. We think it can make a useful tool
for NLP research, and, for that matter, for many other domains where
classification tasks are learned from examples.\\

Memory-Based Learning is a direct descendant of the classical
$k$-Nearest Neighbor ($k$-NN) approach to classification. In typical
NLP learning tasks, however, the focus is on discrete data, very large
numbers of examples, and many attributes of differing
relevance. Moreover, classification speed is a critical issue in any
realistic application of Memory-Based Learning. These constraints,
which are different from those of traditional pattern
recognition applications with their numerical features, often lead to
different data-structures and different speedup optimizations for the
algorithms. Our approach has resulted in an architecture which makes
extensive use of indexes into the instance memory, rather than the
typical flat file organization found in straightforward $k$-NN
implementations. In some cases the internal organization of the memory
results in algorithms which are quite different from $k$-NN, as is the
case with {\sc igtree}. We believe that our optimizations make TiMBL
one of the fastest discrete $k$-NN implementations around.\\

The main effort in the development of this software was done by Ko van
der Sloot. The code started as a rewrite of {\tt nibl}, a piece of
software developed by Peter Berck from a Common Lisp implementation by
Walter Daelemans. Some of the index-optimizations are due to Jakub
Zavrel. The code has benefited substantially from trial, error and
scrutiny by the other members of the ILK group (Sabine Buchholz, Jorn
Veenstra and Bertjan Busser). We would also like to thank Ton Weijters
of Eindhoven Technical University and the members of the {\sc
cnts} research group at the University of Antwerp for their
contributions. This software was written in the context of the
``Induction of Linguistic Knowledge'' research programme, partially
supported by the Foundation for Language Speech and Logic (TSL),
funded by the Netherlands Organization for Scientific Research
(NWO).\\

The current release (version 2.0) is the result of a radical rewrite
from version 1.0, offering a large number of new features. The most
notable changes are:

\begin{itemize}

\item Addition of a new algorithm: {\sc tribl}, a hybrid between the
fast {\sc igtree} algorithm and real nearest neighbor search.

\item Support for numeric features. Although the package has retained
its focus on discrete features, it can now also process numeric
features, do some scaling, and compute feature weights on them.

\item The organization of the code is much more object-oriented than
in version 1.0 (though still not as fully as we'd like to see it).

\item A Memory-Based Learning API. You can define Memory-Based
classification objects in your own software and link to the TiMBL
library.

\end{itemize}

A more elaborate description of the changes from version 1.0 to 2.0
can be found in Chapter~\ref{changes}. Although these new features
have been tested for some time in our research group, the software may
still contains bugs and inconsistencies in some places. The
documentation for the API is still in a very early stage. We would
appreciate it if you would send bug reports, ideas about enhancements
of the software and the manual, and any other comments you might have,
to {\tt Timbl@kub.nl}.\\

This reference guide is structured as follows. In
Chapter~\ref{license} you can find the terms of the license according
to which you are allowed to use TiMBL. The subsequent chapter gives
some instructions on how to install the TiMBL package on your
computer. Chapter~\ref{changes} lists the changes that have taken
place between version 1.0 and 2.0. Readers who are interested in the
theoretical and technical details of Memory-Based Learning and of this
implementation can then proceed to Chapter~\ref{algorithms}. Those who
just want to get started using TiMBL can skip this chapter, and
directly proceed either to Chapters~\ref{fileformats}
and~\ref{commandline}, which respectively provide a reference to the
file formats and command line options of TiMBL, or to
Appendix~\ref{tutorial}, where a short hands-on tutorial is provided
on the basis of a case study with a data set from a linguistic domain
(prediction of Dutch diminutive suffixes). Chapter~\ref{programmers}
gives a specification of the programmer's interface to the TiMBL
library.

\chapter{License terms}
\label{license}
\pagenumbering{arabic} 

Downloading and using the TiMBL software implies that you accept the
following license terms:\\

Tilburg University grants you (the registered user) the non-exclusive
license to download a single copy of the TiMBL program code and
related documentation (henceforth jointly referred to as ``Software'')
and to use the copy of the code and documentation solely in accordance
with the following terms and conditions:

\begin{itemize}

\item The license is only valid when you register as a user. If you
have obtained a copy without registration, you must immediately
register by sending an e-mail to {\tt Timbl@kub.nl}.

\item You may only use the Software for educational or non-commercial
research purposes. 

\item You may make and use copies of the Software internally for your
own use.

\item Without executing an applicable commercial license with Tilburg
University, no part of the code may be sold, offered for sale, or made
accessible on a computer network external to your own or your
organization's in any format; nor may commercial services utilizing
the code be sold or offered for sale. No other licenses are granted or
implied.

\item Tilburg University has no obligation to support the Software it
is providing under this license.  To the extent permitted under the
applicable law, Tilburg University is licensing the Software "AS IS",
with no express or implied warranties of any kind, including, but not
limited to, any implied warranties of merchantability or fitness for
any particular purpose or warranties against infringement of any
proprietary rights of a third party and will not be liable to you for
any consequential, incidental, or special damages or for any claim by
any third party.

\item Under this license, the copyright for the Software remains the
property of the ILK Research Group at Tilburg University.  Except as
specifically authorized by the above licensing agreement, you may not
use, copy or transfer this code, in any form, in whole or in part.

\item Tilburg University may at any time assign or transfer all or
part of its interests in any rights to the Software, and to this
license, to an affiliated or UN-affiliated company or person.

\item Tilburg University shall have the right to terminate this
license at any time by written notice. Licensee shall be liable for
any infringement or damages resulting from Licensee's failure to abide
by the terms of this License.

\item In publication of research that makes use of the Software, a
citation should be given of: {\em ``Walter Daelemans, Jakub Zavrel, Ko
van der Sloot, and Antal van den Bosch (1999). TiMBL: Tilburg Memory
Based Learner, version 2.0, Reference Guide. ILK Technical Report
99-01, Available from {\tt
http://ilk.kub.nl/\~{}ilk/papers/ilk9901.ps.gz}}

\item For information about commercial licenses for the Software,\\
contact {\tt Timbl@kub.nl}, or send your request in writing to:\\

Dr.~Walter Daelemans\\
ILK Research Group\\
Computational Linguistics\\
Tilburg University\\
PO Box 90153\\
5000 LE Tilburg\\
The Netherlands\\

\end{itemize}

\pagestyle{headings}

\chapter{Installation}

You can get the TiMBL package as a gzipped tar archive from:\\
\\
{\tt http://ilk.kub.nl/software.html}\\
\\
Following the links from that page, you will be required to fill in
registration information and to accept the license agreement. You can
then proceed to download the file {\tt Timbl.2.0.tar.gz}\\
This file contains the complete source code (C++) for the TiMBL
program, a few sample data sets, the license and the documentation. The
installation should be relatively straightforward on most UNIX systems.\\
\\
To install the package on your computer, unzip the downloaded file:\\
\\
{\tt > gunzip Timbl.2.0.tar.gz}\\
\\
and unpack the tar archive:\\
\\
{\tt > tar -xvf Timbl.2.0.tar}\\
\\
This will make a directory {\tt Timbl.2.0} under your current directory. Change
directory to this:\\
\\
{\tt > cd Timbl.2.0}\\
\\
and compile the executable by typing {\tt make}\footnote{We have
tested this with {\tt gcc} versions 2.7.2, and 2.8.1}.  If
the process was completed successfully, you should now have an
executable file named {\tt Timbl}.\\
\\
The e-mail address for problems with the installation, bug reports,
comments and questions is {\tt Timbl@kub.nl}.

\chapter{Changes}
\label{changes}

This chapter gives a brief overview of the changes from version 1.0 to
2.0 for users already familiar with the program.

\begin{itemize}

\item We have added of a new algorithm: {\sc tribl}, a hybrid between
the fast {\sc igtree} algorithm and real nearest neighbor search (for
more details, see~\ref{tribl}, or~\cite{Daelemans+97d}). This
algorithm is invoked with the {\tt -a 2} switch and requires the
specification of a so-called {\sc tribl}-offset, the feature where
{\sc igtree} stops and case bases are stored under the leaves of the
constructed tree.

\item Support for numeric features. Although the package has retained
its focus on discrete features, it can now also process numeric
features, scale them, and compute feature weights on them. You
specify which features are numeric with the {\tt -N} option on the
command line.

\item The organization of the code is much more object-oriented than
in version 1.0. The main benefit of this is that:

\item A Memory-Based Learning API is made available. You can define
Memory-Based classification objects in your own C++ programs and
access all of the functionality of TiMBL by linking to the TiMBL
library.

\item It has become easier to examine the way decisions are made from
nearest neighbors, because several verbosity-levels allow you to dump
similarity values ({\tt -D}), distributions ({\tt -v 16}), and nearest
neighbor sets ({\tt -v 32}) to the output file. The {\tt -d} option
for writing the distributions no longer exists.

\item Better support for the manipulation of {\sc mvdm}
matrices. Using the {\tt -U} and {\tt -u} options it is now possible
to respectively save and read back value difference matrices (see
Section~\ref{mvdmformat}).

\item Both ``pre-stored'' and ``regular'' {\sc mvdm} experiments now
generate filenames with ``{\tt mvd}'' in the suffix. This used to be
``{\tt pvd}'' and ``{\tt mvd}'' respectively.

\item a number of minor bugs have been fixed.

\end{itemize}

\chapter{Learning algorithms}
\label{algorithms}

TiMBL is a program implementing several Memory-Based Learning
techniques. All the algorithms have in common that they store some
representation of the training set explicitly in memory. During
testing, new cases are classified by extrapolation from the most
similar stored cases. The main differences between the algorithms
incorporated in TiMBL lie in:

\begin{itemize}
\item The definition of {\em similarity},
\item The way the instances are stored in memory, and
\item The way the search through memory is conducted.
\end{itemize}

In this chapter, various choices for these issues are described. We
start in section~\ref{mbl} with a formal description of the basic
Memory-Based Learning algorithm, i.e.~a nearest neighbor search. We
then introduce different similarity metrics, such as Information Gain
weighting, which allows us to deal with features of differing
importance, and the Modified Value Difference metric, which allows us
to make a graded guess of the match between two different symbolic
values. In section~\ref{tree-base} and~\ref{inverse-index}, we give a
description of various optimizations for nearest neighbor search. In
section~\ref{igtree}, we describe the fastest optimization, {\sc
igtree}, which replaces the exact nearest neighbor search with a very
fast heuristic that exploits the difference in importance between
features. Finally, in section~\ref{tribl}, we describe the {\sc tribl}
algorithm, which is a hybrid between {\sc igtree} and nearest neighbor
search.

\section{Memory Based Learning}
\label{mbl}

Memory-based learning is founded on the hypothesis that performance in
cognitive tasks is based on reasoning on the basis of similarity of
new situations to {\em stored representations of earlier experiences},
rather than on the application of {\em mental rules}\/ abstracted from
earlier experiences (as in rule induction and rule-based processing).
The approach has surfaced in different contexts using a variety of
alternative names such as similarity-based, example-based,
exemplar-based, analogical, case-based, in\-stance-based, and lazy
learning~\cite{Stanfill+86,Cost+93,Kolodner93,Aha+91,Aha97a}.
Historically, memory-based learning algorithms are descendants of the
$k$-nearest neighbor (henceforth $k$-{\sc nn}) algorithm
\cite{Cover+67,Devijver+82,Aha+91}.

An {\sc mbl} system, visualized schematically in
Figure~\ref{mbl-method}, contains two components: a {\em learning
component}\/ which is memory-based (from which {\sc mbl} borrows its
name), and a {\em performance component}\/ which is similarity-based.

The learning component of {\sc mbl} is memory-based as it involves
adding training instances to memory (the {\em instance base} or case
base); it is sometimes referred to as `lazy' as memory storage is done
without abstraction or restructuring.  An instance consists of a
fixed-length vector of $n$ feature-value pairs, and an information
field containing the classification of that particular feature-value
vector.  

In the performance component of an {\sc mbl} system, the product of
the learning component is used as a basis for mapping input to output;
this usually takes the form of performing classification.  During
classification, a previously unseen test example is presented to the
system. The similarity between the new instance $X$ and all examples
$Y$ in memory is computed using a {\em distance metric}
$\Delta(X,Y)$. The extrapolation is done by assigning the most
frequent category within the $k$ most similar example(s) as the
category of the new test example.

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=8cm
                \epsffile{mble-method.eps}
                \caption{General architecture of an {\sc mbl} system.
                }
                \label{mbl-method}
        \end{center}
\end{figure}

\subsection{Overlap metric}
\label{overlap}

The most basic metric that works for patterns with symbolic features
is the {\bf Overlap metric}\footnote{This metric is also referred to
as Hamming distance, Manhattan metric, city-block distance, or L1
metric.} given in equations~\ref{distance} and~\ref{overlapeq}; where
$\Delta(X,Y)$ is the distance between patterns $X$ and $Y$,
represented by $n$ features, and $\delta$ is the distance per
feature. The distance between two patterns is simply the sum of the
differences between the features. The $k$-{\sc nn} algorithm with this
metric is called {\sc ib1} \cite{Aha+91}. Usually $k$ is set to 1.

\begin{equation}
\Delta(X,Y) = \sum_{i=1}^{n} \delta(x_{i},y_{i})
\label{distance}
\end{equation}

where:
\begin{equation}
\delta(x_{i}, y_{i}) = \left\{ \begin{array}{ll}
		\frac{x_{i}-y_{i}}{max_{i}-min_{i}} & \mbox{if numeric, else}\\
		0 & \mbox{if $x_{i} = y_{i}$}\\
		1 & \mbox{if $x_{i} \neq y_{i}$}\\
	\end{array} \right.
\label{overlapeq}
\end{equation}

We have made three additions to the original algorithm \cite{Aha+91} in
our version of {\sc ib1}.  First, in the case of nearest neighbor sets
larger than one instance ($k>1$ or ties), our version of {\sc ib1}
selects the classification that has the highest frequency in the class
distribution of the nearest neighbor set. Second, if a tie cannot be
resolved in this way because of equal frequency of classes among the
nearest neighbors, the classification is selected with the highest
overall occurrence in the training set. Third, in our implementation,
the value of $k$ refers to $k$-nearest distances rather than
$k$-nearest cases.

\subsection{Information Gain weighting}
\label{infogain}

The distance metric in equation~\ref{overlapeq} simply counts the
number of (mis)matching feature-values in both patterns. In the
absence of information about feature relevance, this is a reasonable
choice. Otherwise, we can add domain knowledge bias to weight or
select different features (see e.g.~\cite{Cardie96} for an
application of linguistic bias in a language processing task), or look
at the behavior of features in the set of examples used for
training. We can compute statistics about the relevance of features by
looking at which features are good predictors of the class
labels. Information Theory gives us a useful tool for measuring
feature relevance in this way~\cite{Quinlan86,Quinlan93}.

{\bf Information Gain} (IG) weighting looks at each feature in
isolation, and measures how much information it contributes to our
knowledge of the correct class label. The Information Gain of feature
$i$ is measured by computing the difference in uncertainty
(i.e. entropy) between the situations without and with knowledge of
the value of that feature (equation~\ref{IGgain}).

\begin{equation}
w_{i} = H(C) -  \sum_{v \in V_{i}} P(v) \times H(C|v)
\label{IGgain}
\end{equation}

Where $C$ is the set of class labels, $V_{i}$ is the set of values for
feature $i$, and $H(C) = - \sum_{c \in C} P(c) \log_{2} P(c)$ is the
entropy of the class labels. The probabilities are estimated from
relative frequencies in the training set. For numeric features, values
are first discretized into a number (the default is 20) of equally
spaced intervals between the minimum and maximum values of the
feature. These groups are then used in the IG computation as if they
were discrete values (note that this discretization is not used in the
computation of the distance metric).

It is important to realize that the IG weight is really a probability
weighted average of the informativity of the different values of the
feature. On the one hand, this pre-empts the consideration of values
with low frequency but high informativity. Such values ``disappear''
in the average. On the other hand, this also makes the IG weight very
robust to estimation problems. Each parameter (=weight) is estimated
on the whole data set.

Information Gain, however, tends to overestimate the relevance of
features with large numbers of values. Imagine a data set of hospital
patients, where one of the available features is a unique ``patient ID
number''. This feature will have very high Information Gain, but it
does not give any generalization to new instances. To normalize
Information Gain for features with different numbers of values,
Quinlan~\cite{Quinlan93} has introduced a normalized version, called
Gain Ratio, which is Information Gain divided by $si(i)$ (split info),
the entropy of the feature-values (equation~\ref{splitinfo}).

\begin{equation}
w_{i} = \frac{H(C) -  \sum_{v \in V_{i}} P(v) \times H(C|v)}{si(i)}
\label{IGgainratio}
\end{equation}

\begin{equation}
si(i) = - \sum_{v \in V_{i}} P(v) \log_{2} P(v)
\label{splitinfo}
\end{equation}

The resulting Gain Ratio values can then be used as weights $w_{f}$ in
the weighted distance metric (equation~\ref{distancew})\footnote{In a
generic use IG refers both to Information Gain and to Gain Ratio
throughout this manual. In specifying parameters for the software, the
distinction between both needs to be made, because they often result
in different behavior.}. The $k$-{\sc nn} algorithm with this
metric is called {\sc ib1-ig} \cite{Daelemans+92b}.

\begin{equation}
\Delta(X,Y) = \sum_{i=1}^{n}\ w_{i} \ \delta(x_{i},y_{i})
\label{distancew}
\end{equation} 

The possibility of automatically determining the relevance of features
implies that many different and possibly irrelevant features can be
added to the feature set. This is a very convenient methodology if
domain knowledge does not constrain the choice enough beforehand, or
if we wish to measure the importance of various information sources
experimentally. However, because IG values are computed for each
feature independently, this is not necessarily the best
strategy. Sometimes better results can be obtained by leaving features
out than by letting them in with a low weight. Very redundant features
can also be challenging for {\sc ib1-ig}, because IG will overestimate
their joint relevance. Imagine an informative feature which is
duplicated. This results in an overestimation of IG weight by a factor
two, and can lead to accuracy loss, because the doubled feature will
dominate the similarity metric.

\subsection{Modified Value Difference metric}
\label{mvdm}

It should be stressed that the choice of representation for instances
in {\sc mbl} remains the key factor determining the strength of the
approach. The features and categories in NLP tasks are usually
represented by symbolic labels. The metrics that have been described
so far, i.e.~Overlap and IG Overlap, are limited to exact match
between feature-values. This means that all values of a feature are
seen as equally dissimilar. However, if we think of an imaginary task
in e.g.~the phonetic domain, we might want to use the information that
'b' and 'p' are more similar than 'b' and 'a'. For this purpose a
metric was defined by Stanfill \& Waltz~\cite{Stanfill+86} and further
refined by Cost \& Salzberg~\cite{Cost+93}. It is called the
(Modified) Value Difference Metric ({\sc mvdm}; equation~\ref{MVDMeq}), and
it is a method to determine the similarity of the values of a feature
by looking at co-occurrence of values with target classes. For the
distance between two values $V_{1},\ V_{2}$ of a feature, we compute
the difference of the conditional distribution of the classes $C_{i}$
for these values.

\begin{equation}
\delta(V_{1}, V_{2}) = \sum_{i=1}^{n} \left| P(C_{i}|V_{1}) - P(C_{i}|V_{2})
\right|
\label{MVDMeq}
\end{equation}

For computational efficiency, all pairwise $\delta(V_{1}, V_{2})$
values can be pre-comput\-ed before the actual nearest neighbor search
starts. Note that for numeric features, no {\sc mvdm} is computed in
TiMBL, but a scaled difference (see Equation~\ref{overlapeq}) of the
actual numeric feature values.

Although the {\sc mvdm} metric does not explicitly compute feature
relevance, an implicit feature weighting effect is present. If
features are very informative, their conditional class probabilities
will on average be very skewed towards a particular class. This
implies that on average the $\delta(V_{1}, V_{2})$ will be large. For
uninformative features, on the other hand, the conditional class
probabilities will be pretty uniform, so that on average the
$\delta(V_{1}, V_{2})$ will be very small.

{\sc mvdm} differs considerably from Overlap based metrics in its
composition of the nearest neighbor sets. Overlap causes an abundance
of ties in nearest neighbor position. For example, if the nearest
neighbor is at a distance of one mismatch from the test instance, then
the nearest neighbor set will contain the entire partition of the
training set that matches all the other features but contains {\em
any} value for the mismatching feature (see~\cite{Zavrel+97} for a
more detailed discussion). With the {\sc mvdm} metric, however, the
nearest neighbor set will either contain patterns which have the value
with the lowest $\delta(V_{1}, V_{2})$ in the mismatching position, or
{\sc mvdm} will select a totally different nearest neighbor which has
less exactly matching features, but a smaller distance in the
mismatching features. In sum, this
means that the nearest neighbor set is usually much smaller for {\sc
mvdm} at the same value of $k$. In NLP tasks we have found it very
useful to experiment with values of $k$ larger than one for {\sc
mvdm}, because this re-introduces some of the beneficial smoothing
effects associated with large nearest neighbor sets.

One cautionary note about this metric is connected to data
sparsity. In many practical applications, we are confronted with a
very limited set of examples. This poses a serious problem for the
{\sc mvdm} metric. Many values occur only once in the whole data
set. This means that if two such values occur with the same class the
{\sc mvdm} will regard them as identical, and if they occur with two
different classes their distance will be maximal. The latter condition
reduces the {\sc mvdm} to the Overlap metric for many cases, with the
addition that some cases will be counted as an exact match or mismatch
on the basis of very shaky evidence.

\section{Tree-based memory}
\label{tree-base}

The discussion of the algorithm and the metrics in the section above
is based on a naive implementation of nearest neighbor search: a flat
array of instances which is searched from beginning to end while
computing the similarity of the test instance with each training
instance (see the left part of Figure~\ref{example1}). Such an
implementation, unfortunately, reveals the flip side of the lazy
learning coin. Although learning is very cheap: just storing the
instances in memory, the computational price of classification can
become very high for large data sets. The computational cost is
proportional to $N$, the number of instances in the training set.

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=\columnwidth
                \epsffile{example1.eps}
                \caption{The instance base for a small object
                classification toy problem. The left figure shows a
                flat array of instances through which sequential
                nearest neighbor search is performed to find the best 
		match for a test instance (shown below the instance
                base). 
		In the right part, an inverted index (see text)
                is used to restrict the search to those instances
                which share at least one feature value with the test
                instance.
                }
                \label{example1}
        \end{center}
\end{figure}

In our implementation of {\sc ib1} we use a more efficient
approach. The first part of this approach is to replace the flat array
by a tree-based data structure. Instances are stored in the tree as
paths from a root node to a leaf, the arcs of the path are the
consecutive feature-values, and the leaf node contains a {\em
distribution} of classes, i.e.~a count of how many times which class
occurs with this pattern of feature-values (see Figure~\ref{example2}).

Due to this storage structure, instances with identical feature-values
are collapsed into one path, and only their separate class
information needs to be stored in the distribution at the leaf
node. Many different {\bf tokens} of a particular {\bf instance type}
share one path from the root to a leaf node. Moreover, instances which
share a prefix of feature-values, also share a partial path. This
reduces storage space (although at the cost of some book-keeping
overhead) and has two implications for nearest neighbor search
efficiency.

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=10cm
                \epsffile{example2.eps}\
                \caption{A tree-structured storage of the instance
                base from figure~\ref{example1}. An exact match for
                the test is in this case directly found by a top down
                traversal of the tree (grey path). If there is no
                exact match, all paths are interpreted as instances
                and the distances are computed. The order of the
                features in this tree is based on Gain Ratio.
                }
                \label{example2}
        \end{center}
\end{figure}


In the first place, the tree can be searched top-down very quickly for
{\em exact matches}. Since an exact match ($\Delta(X,Y)=0$) can never
be beaten, we choose to omit any further distance computations when
one is found with this shortcut\footnote{There is a command line
switch ({\tt -x}) which turns the shortcut off in order to get real $k$-NN
results when $k>1$ (i.e. get neighbors at further distances).}.

Second, the distance computation for the nearest neighbor
search can re-use partial results for paths which share prefixes.
This re-use of partial results is in the direction from the root to
the leaves of the tree. When we have proceeded to a certain level of
the tree, we know how much similarity (equation~\ref{overlapeq}) can
still contribute to the overall distance (equation~\ref{distance}),
and discard whole branches of the tree which will never be able to
rise above the partial similarity of the current least similar best
neighbor. 

Disregarding this last constraint on search, the number of feature-value
comparisons is equal to the number of arcs in the tree. Thus if we can
find an ordering of the features which produces more overlap between
partial paths, and hence a smaller tree, we can gain both space and
time improvements. An ordering which was found to produce small trees
for many of our NLP data sets is Gain Ratio divided by the number of
feature-values (this is the default setting). Through the {\tt -T}
command line switch, however, the user is allowed to experiment with
different orderings.

\section{Inverse index}
\label{inverse-index}

The second part of our approach to efficiency is a speedup
optimization based on the following fact. Even in the tree-based
structure, the distance is computed between the test instance and {\em
all} instance types. This means that even instance types which do not
share a single feature-value with the test instance are considered,
although they will surely yield a zero similarity. The use of an {\bf
inverted index} excludes these zero similarity patterns.  The
construction of the inverted index records for all values of each
feature a list of instance types (i.e. leaf nodes in the tree
described in the previous section) in which they occur. Thus it is an
inverse of the instance-base, which records for each instance type
which feature-values occur in it\footnote{Unfortunately this also
implies that the storage of both an instance-base and an inverted
index takes about twice the amount of memory.}.

When a test instance is to be classified, we select the lists of
instance types for the feature-values that it contains (illustrated in
the rightmost part of Figure~\ref{example1}). We can now find the
nearest neighbor in these lists in a time that is proportional to the
number of occurrences of the most frequent feature-value of the test
pattern, instead of proportional to the number of instance types.

Although worst case complexity is still proportional to $N$, the size
of the training set, and practical mileage may vary widely depending
on the peculiarities of your data, the combination of exact match
shortcut, tree-based path re-use, and inverted index has proven in
practice (for our NLP datasets) to make the difference between hours
and seconds of computation\footnote{{\sc mvdm} and numeric features
cannot make use of the inverted index optimization, because it can
happen that two cases with not one value in common are still nearest
neighbors. 
%
%Because the precomputation of differences between
%values is often impossible in tasks with a large number of feature
%values ($n^2$ differences must be stored per feature, if $n$ is the
%number of values of that feature), and because {\sc mvdm} then
%effectively multiplies the number of distance computations per
%instance by the number of classes, this metric is currently one of the
%slowest in the package.
}.

\section{IGTree}
\label{igtree}

Using Information Gain rather than unweighted Overlap distance to
define similarity in {\sc ib1} improves its performance on several
{\sc nlp} tasks \cite{Daelemans+92b,VandenBosch+93,VandenBosch97}.
The positive effect of Information Gain on performance prompted us to
develop an alternative approach in which the instance memory is
restructured in such a way that it contains the same information as
before, but in a compressed decision tree structure. We call this
algorithm {\sc igtree}~\cite{Daelemans+97} (see Figure~\ref{example3}
for an illustration). In this structure, similar to the
tree-structured instance base described above, instances are stored as
paths of connected nodes which contain classification
information. Nodes are connected via arcs denoting feature
values. Information Gain is used to determine the order in which
instance feature-values are added as arcs to the tree. The reasoning
behind this compression is that when the computation of information
gain points to one feature clearly being the most important in
classification, search can be restricted to matching a test instance
to those memory instances that have the same feature-value as the test
instance at that feature. Instead of indexing all memory instances
only once on this feature, the instance memory can then be optimized
further by examining the second most important feature, followed by
the third most important feature, etc.  Again, considerable
compression is obtained as similar instances share partial paths.

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=10cm
                \epsffile{example3.eps}
                \caption{A pruned {\sc igtree} for the instance base
                of Figure~\ref{example1}. The classification for
                the test instance is found by top down search of the
                tree, and returning the class label (default) of the
                node after the last matching feature-value (arc). Note
                that this tree is essentially a compressed version of
                the tree in Figure~\ref{example2}.
                }
                \label{example3}
        \end{center}
\end{figure}

Because {\sc igtree} makes a heuristic approximation of nearest
neighbor search by a top down traversal of the tree in the order of
feature relevance, we no longer need to store all the paths. The idea
is that it is not necessary to fully store those feature-values of the
instance that have lower Information Gain than those features which
already fully disambiguate the instance classification.

Apart from compressing all training instances in the tree structure,
the {\sc igtree} algorithm also stores with each non-terminal node
information concerning the {\em most probable} or {\em default}
classification given the path thus far, according to the bookkeeping
information maintained by the tree construction algorithm. This extra
information is essential when processing unknown test instances.
Processing an unknown input involves traversing the tree (i.e.,
matching all feature-values of the test instance with arcs in the
order of the overall feature Information Gain), and either retrieving
a classification when a leaf is reached (i.e., an exact match was
found), or using the default classification on the last matching
non-terminal node if an exact match fails.

In sum, it can be said that in the trade-off between computation
during learning and computation during classification, the {\sc
igtree} approach chooses to invest more time in organizing the
instance base using Information Gain and compression, to obtain
considerably simplified and faster processing during classification,
as compared to {\sc ib1} and {\sc ib1-ig}.
 
The generalization accuracy of {\sc igtree} is usually comparable to
that of {\sc ib1-ig}; most of the time not significantly differing,
and occasionally slightly (but statistically significantly) worse, or
even better.  The two reasons for this surprisingly good accuracy are
that (i) most `unseen' instances contain considerably large parts that
fully match stored parts of training instances, and (ii) the
probabilistic information stored at non-terminal nodes (i.e., the
default classifications) still produces strong `best guesses' when
exact matching fails. The difference between the top-down traversal of
the tree and precise nearest neighbor search becomes more pronounced
when the differences in informativity between features are small. In
such a case a slightly different weighting would have produced a
switch in the ordering and a completely different tree. The result can
be a considerable change in classification outcomes, and hence also in
accuracy. However, we have found in our work on NLP datasets that when
the goal is to obtain a very fast classifier for processing large
amounts of text, the slight tradeoff between accuracy and speed can be
very attractive. Note, also, that by design, {\sc igtree} is not suited
for numeric features, as long as it does not use some type of
discretization. In TiMBL numbers will simply be treated as literal
strings in this case.

\section{The TRIBL hybrid}
\label{tribl}

The application of {\sc igtree} on a number of common machine-learning
datasets suggested that it is not applicable to problems where the
relevance of the predictive features cannot be ordered in a
straightforward way, e.g.~if the differences in Information Gain are
only very small. In those cases, {\sc ib1-ig} or even {\sc ib1} tend
to perform significantly better than {\sc igtree}.

For this reason we have designed {\sc tribl}, a hybrid generalization
of {\sc igtree} and {\sc ib1}. {\sc tribl} allows you to exploit the
trade-off between (i) optimization of search speed (as in {\sc
igtree}), and (ii) maximal generalization accuracy. To achieve this, a
parameter is set determining the switch from {\sc igtree} to {\sc
ib1}. A heuristic that we have used with some success is based on {\em
average feature information gain}; when the Information Gain of a
feature exceeds the sum of the average Information Gain of all
features $+$ one standard deviation of the average, then the feature
is used for constructing an {\sc igtree}, including the computation of
defaults on nodes. When the Information Gain of a feature is below
this threshold, and the node is still ambiguous, tree construction
halts and the leaf nodes at that point represent case bases containing
subsets of the original training set. During search, the normal {\sc
igtree} search algorithm is used, until the case-base nodes are
reached, in which case regular {\sc ib1} nearest neighbor search is
used on this sub-case-base. In TiMBL, however, you must specify the
switch point from {\sc igtree} to {\sc ib1}, also referred to as
``{\sc tribl} offset'', manually.

\section{NLP applications of TiMBL}
\label{furtherreading}

This section provides a historical overview of our own work with the
application of {\sc mbl} type algorithms to NLP tasks.

 The {\sc ib1-ig} algorithm was first introduced in
\cite{Daelemans+92b} in the context of a comparison of memory-based
approaches with backprop learning for a hyphenation task.  Predecessor
versions of {\sc igtree} can be found in
\cite{Daelemans+93c,VandenBosch+93} where they are applied to
grapheme-to-phoneme conversion.  See \cite{Daelemans+97} for a
detailed description and review of the algorithms.  A recent
development, now implemented in the TiMBL package is {\sc tribl}
\cite{Daelemans+97d}.

The memory-based algorithms implemented in the TiMBL package have been
successfully applied to a large range of Natural Language Processing
tasks: hyphenation and syllabification (\cite{Daelemans+92b});
assignment of word stress (\cite{Daelemans+94b}); grapheme-to-phoneme
conversion (\cite{Daelemans+96}); diminutive formation
(\cite{Daelemans+97e}); morphological analysis
(\cite{Vandenbosch+96}); part of speech tagging
(\cite{Daelemans+96b}); PP-attachment (\cite{Zavrel+97b}); word sense
disambiguation(\cite{Veenstra+98}); subcategorization
(\cite{Buchholz98}); and chunking (partial parsing)
(\cite{Veenstra98}).

Relations to statistical language processing are discussed in
\cite{Zavrel+97}.  A partial overview paper is \cite{Daelemans95}.
The first dissertation-length study devoted to the approach is
\cite{VandenBosch97}, in which the approach is compared to alternative
learning methods for NLP tasks related to English word pronunciation
(stress assignment, syllabification, morphological analysis,
alignment, grapheme-to-phoneme conversion).

All papers referred to in this section are available in electronic
form from the {\sc ILK} homepage: {\tt http://ilk.kub.nl}.  We are
grateful for any feedback on the algorithms and the way we applied
them.

Whereas the work in Tilburg has been oriented primarily towards {\em
language engineering} applications, the {\sc cnts} research group of
Antwerp University, with which close research ties exist, has studied
the linguistic and psycholinguistic relevance of memory-based learning
for stress assignment in Dutch (\cite{Daelemans+94b,Gillis+95}), and
as a model for {\em phonological bootstrapping}.  A recently started
project has as its aim to test predictions from memory-based models
for language processing with psycholinguistic experiments.

\chapter{File formats}
\label{fileformats}

This chapter describes the format of the input and output files used
by TiMBL. Where possible, the format is illustrated using the same
small toy data set that is shown in Figure~\ref{example1}. It consists
of 12 instances of 5 different everyday objects (nut, screw, key, pen,
scissors), described by 3 discrete features (size, shape, and number
of holes).

\section{Data format}
\label{dataformats}

The training and test sets for the learner consist of descriptions of
instances in terms of a fixed number of feature-values. TiMBL supports
a number of different formats for the instances, but they all have in
common that the files should contain one instance per line. The number
of instances is determined automatically, and the format of each
instance is inferred from the format of the first line in the training
set. The last feature of the instance is assumed to be the target
category. Should the guess of the format by TiMBL turn out to be
wrong, you can force it to interpret the data as a particular format
by using the {\tt -F} option. Note that TiMBL, by default, will
interpret features as having {\em symbolic, discrete values}. Unless
you specify explicitly that certain features are numeric, using the
{\tt -N} option, TiMBL will interpret numbers as just another string
of characters. If a feature is numeric, its values will be scaled to
the interval [0,1] for purposes of distance computation (see
Equation~\ref{overlapeq}). The computation of feature weights will be
based on a discretization of the feature.

Once TiMBL has determined the input format, it will skip and complain
about all lines in the input which do not respect this format
(e.g.~have a different number of feature-values with respect to that
format).

During testing, TiMBL writes the classifications of the test set to an
output file. The format of this output file is by default the same as
the input format, with the addition of the predicted category being
appended after the correct category. If we turn on higher levels of
verbosity, the output files will also contain distributions, distances
and nearest neighbor sets.

\subsection{Column format}
\label{comlumnformat}

The {\bf column format} uses white space as the separator between
features. White space is defined as a sequence of one or more spaces or
tab characters. Every instance of white space is interpreted as a
feature separator, so it is not possible to have feature-values
containing white space. The column format is auto-detected when an
instance of white space is detected on the first line {\em before a
comma has been encountered}. The example data set looks like this in
the column format:

\begin{verbatim}
small	compact	1	nut
small	long	none	screw
small	long	1	key
small	compact	1	nut
large	long	1	key
small	compact	none	screw
small	compact	1	nut
large	long	none	pen
large	long	2	scissors
large	long	1	pen
large	other	2	scissors
small	other	2	key
\end{verbatim}

\subsection{C4.5 format}
\label{c45format}

This format is a derivative of the format that is used by the
well-known C4.5 decision tree learning program~\cite{Quinlan93}.  The
separator between the features is a comma, and the category (viz.  the
last feature on the line) is followed by a period (although this is
not mandatory: TiMBL is robust to missing periods)\footnote{The
periods after the category are not reproduced in the output}.  White
space within the line is taken literally, so the pattern {\tt a, b
c,d} will be interpreted as {\tt `a',` b c',`d'}.  When using this
format, especially with linguistic data sets or with data sets
containing floating point numbers, one should take special care that
commas do not occur in the feature-values and that periods do not
occur within the category.  Note that TiMBL's C4.5 format does not
require a so called {\em namesfile}.  However, TiMBL can produce such
a file for C4.5 with the {\tt -n} option.  The C4.5 format is
auto-detected when a comma is detected on the first line {\em before
any white space has been encountered}. The example data set looks like
this in the C4.5 format:

\begin{verbatim}
small,compact,1,nut.
small,long,none,screw.
small,long,1,key.
small,compact,1,nut.
large,long,1,key.
small,compact,none,screw.
small,compact,1,nut.
large,long,none,pen.
large,long,2,scissors.
large,long,1,pen.
large,other,2,scissors.
small,other,2,key.
\end{verbatim}

\subsection{ARFF format}
\label{arffformat}

ARFF is a format that is used by the WEKA machine learning
workbench~\cite{Garner95}\footnote{WEKA is available from the Waikato
University Department of Computer Science, {\tt
http://www.cs.waikato.ac.nz/\~{}ml/}.}.  Although TiMBL at present
does {\em not} entirely follow the ARFF specification, it still tries to do
as well as it can in reading this format. In ARFF the actual data are
preceded by a header with various types of information and
interspersed with lines of comments (starting with \%). The ARFF
format is auto-detected when the first line starts with \% or @. TiMBL
ignores lines with ARFF comments and instructions, and starts reading
data from after the {\tt @data} statement until the end of the
file. The feature-values are separated by commas, and white space is
deleted entirely, so the pattern {\tt a, b c,d} will be interpreted as
{\tt `a',`bc',`d'}. We plan to include better support for the
ARFF format in future releases.

\begin{verbatim}
% There are 4 attributes.
% There are 12 instances.
% Attribute information:                       Ints Reals  Enum  Miss
%          'size'                                 0     0    12     0   
%          'shape'                                0     0    12     0   
%          'n_holes'                              9     0     3     0   
%          'class.'                               0     0    12     0   
@relation 'example.data'
@attribute 'size' { small, large}
@attribute 'shape' { compact, long, other}
@attribute 'n_holes' { 1, none, 2}
@attribute 'class.' { nut., screw., key., pen., scissors.}
@data
small,compact,1,nut.
small,long,none,screw.
small,long,1,key.
small,compact,1,nut.
large,long,1,key.
small,compact,none,screw.
small,compact,1,nut.
large,long,none,pen.
large,long,2,scissors.
large,long,1,pen.
large,other,2,scissors.
small,other,2,key.
\end{verbatim}

\subsection{Compact format}
\label{compactformat}

The compact format is especially useful when dealing with very large
data files. Because this format does not use any feature separators,
file-size is reduced considerably in some cases. The price of this is
that all features and class labels must be of equal length (in
characters) and TiMBL needs to know beforehand what this length
is. You must tell TiMBL by using the {\tt -l} option. The compact
format is auto-detected when neither of the other formats applies. The
same example data set might look like this in the column format (with
two characters per feature):

\begin{verbatim}
smco1_nu
smlonosc
smlo1_ke
smco1_nu
lalo1_ke
smconosc
smco1_nu
lalonope
lalo2_sc
lalo1_pe
laot2_sc
smot2_ke
\end{verbatim}

\section{Weight files}
\label{weightformat}

The feature weights that are used for computing similarities and for
the internal organization of the memory-base can be saved to a file.
A file with weights can be constructed or altered manually and then
read back into TiMBL. The format for the weights file is as follows.
The weights file may contain comments on lines that start with a \#
character. The other lines contain the number of the feature followed
by its numeric weight. An example of such a file is provided
below. The numbering of the weights starts with 1 and follows the same
order as in the data file. If features are to be ignored it is
advisable not to set them to zero, but give them the value ``{\tt
Ignored}'' or to use the {\tt -s} option.

\begin{verbatim}
# DB Entropy: 2.29248
# Classes: 5
# Lines of data: 12
# Fea.  Weight
1       0.765709
2       0.614222
3       0.73584
\end{verbatim}

\section{Value difference files}
\label{mvdmformat}

Using the {\sc mvdm} metric, it can sometimes be interesting to
inspect the matrix of conditional class probabilities from
Equation~\ref{MVDMeq}. By using the {\tt -U} option, we can write the
computed matrix to a file. This way we can see which values are
considered to be similar by the metric. For each feature a row vector
is given for each value, of the conditional probabilities of all the
classes given that value.

\begin{verbatim}
feature # 1 Matrix: 
small   0.429   0.286   0.286   0.000   0.000
large   0.000   0.000   0.200   0.400   0.400
 
feature # 2 Matrix: 
compact 0.750   0.250   0.000   0.000   0.000
long    0.000   0.167   0.333   0.333   0.167
other   0.000   0.000   0.500   0.000   0.500
 
feature # 3 Matrix: 
1       0.500   0.000   0.333   0.167   0.000
none    0.000   0.667   0.000   0.333   0.000
2       0.000   0.000   0.333   0.000   0.667
\end{verbatim}

As long as this format is observed, the file can be modified (manually
or by substituting some other vector-based representations for the
values), and the new matrix can be read in and used with the {\sc
mvdm} metric.

\section{Tree files}
\label{treeformat}

Although the learning phase in TiMBL is relatively fast, it can
sometimes be useful to store the internal representation of the data
set for even faster subsequent retrieval. In TiMBL, the data set is
stored internally in a tree structure (see
Section~\ref{tree-base}). When using {\sc ib1}, this tree representation
contains all the training cases as full paths in the tree. When using
{\sc igtree}, unambiguous paths in the tree are pruned before it is
used for classification or written to file. In either tree, the arcs
represent feature-values and nodes contain class (frequency
distribution) information. The features are in the same order
throughout the tree. This order is either determined by memory-size
considerations in {\sc ib1}, or by feature relevance in {\sc igtree}. It can
explicitly be manipulated using the {\tt -T} option.

We strongly advise to refrain from manually editing the tree
file. However, the syntax of the tree file is as follows. After a
header consisting of information about the status of the tree, the
feature-ordering (the permutation from the order in the data file to
the order in the tree), and the presence of numeric
features\footnote{Although in this header each line starts with '\#',
these lines cannot be seen as comment lines.} the tree's nodes and
arcs are given in non-indented bracket notation.

Starting from the root node, each node is denoted by an opening
parenthesis ``('', followed by a default class. After this, there is
the class distribution list, within curly braces ``\{ \}'', containing
a non-empty list of categories followed by integer counts. After this
comes an optional list of children, within ``[ ]'' brackets,
containing a non-empty list of nodes. The choice whether distributions
are present is maintained throughout the whole tree. Whether children
are present is really dependent on whether children {\em are} present.

The {\sc ib1} tree that was constructed from our example data set looks as
follows:

\begin{verbatim}
# Status: complete
# Permutation: < 1, 3, 2 >
# Numeric: .
#
( nut { nut 3 screw 2 key 3 pen 2 scissors 2 }
 [ small ( nut { nut 3 screw 2 key 2 }
 [ 1 ( nut { nut 3 key 1 }
 [ compact ( nut { nut 3 }
 )
long ( key { key 1 }
 )
 ]
 )
none ( screw { screw 2 }
 [ compact ( screw { screw 1 }
 )
long ( screw { screw 1 }
 )
 ]
 )
2 ( key { key 1 }
 [ other ( key { key 1 }
 )
q ]
 )
 ]
 )
large ( pen { key 1 pen 2 scissors 2 }
 [ 1 ( key { key 1 pen 1 }
 [ long ( key { key 1 pen 1 }
 )
 ]
 )
none ( pen { pen 1 }
 [ long ( pen { pen 1 }
 )
 ]
 )
2 ( scissors { scissors 2 }
 [ long ( scissors { scissors 1 }
 )
other ( scissors { scissors 1 }
 )
 ]
 )
 ]
 )
 ]
 )
\end{verbatim}

The corresponding compressed {\sc igtree} version is much smaller. 

\begin{verbatim}
# Status: pruned
# Permutation: < 1, 3, 2 >
# Numeric: .
#
( nut { nut 3 screw 2 key 3 pen 2 scissors 2 } 
 [ small ( nut { nut 3 screw 2 key
 2 } 
 [ 1 ( nut { nut 3 key 1 } [ long ( key { key 1 } )
 ]
 )
none ( screw { screw 2 } )
2 ( key { key 1 } )
 ]
 )
large ( pen { key 1 pen 2 scissors 2 } 
 [ 1 ( key { key 1 pen 1 } )
2 ( scissors { scissors 2 } )
 ]
 )
 ]
 )
\end{verbatim}

\chapter{Command line options}
\label{commandline}

The user interacts with TiMBL through the use of command line arguments.
When you have installed TiMBL successfully, and you type {\tt Timbl} at the
command line without any further arguments, it will print an overview
of the most basic command line options. 

\begin{verbatim}
TiMBL Version 2.0, (c) ILK 1998,1999.
Tilburg Memory Based Learner
Induction of Linguistic Knowledge Research Group, Tilburg University.

usage:  Timbl -f data-file {-t test-file}
or see: Timbl -h
        for all possible options
\end{verbatim}

If you are satisfied with all of the default settings, you can proceed
with just these basics:

\begin{description}

\item {\tt -f <datafile>} : supplies the name of the file with the training items.
\item {\tt -t <testfile>} : supplies the name of the file with the 
test items.
\item {\tt -h} : prints a glossary of all available command line 
options.

\end{description}

The presence of a training file will make TiMBL pass through the first
two phases of its cycle. In the first phase it examines the contents
of the training file, and computes a number of statistics on it
(feature weights etc.). In the second phase the instances from the
training file are stored in memory. If no test file is specified, the
program exits, possibly writing some of the results of learning to
files (see below). If there is a test file, the selected classifier,
trained on the present training data, is applied to it, and the
results are written to a file of which name is a combination of the
name of the test file and a code representing the chosen algorithm
settings. TiMBL then reports the percentage of correctly classified
test items. The default settings for the classification phase are: a
Memory-Based Learner, with Gain Ratio feature weighting, with $k=1$,
and with optimizations for speedy search. If you need to change the
settings, because you want to use a different type of classifier, or
because you need to make a trade-off between speed and memory-use,
then you can use the options that are shown using {\tt -h}. The
sections below provide a reference to the use of these command line
arguments, and they are roughly ordered by the type of action that the
option has effect on.

\section{Algorithm and Metric selection}

\begin{description}

\item {\tt -a <n>} : chooses between the standard {\sc ib1} (nearest
neighbor search) algorithm (n=0, this is the default value), the
decision tree-based optimization {\sc igtree} (n=1), and the hybrid of
the two: {\sc tribl} (n=2).

\item {\tt -m <n>} : chooses between similarity metrics. Only
applicable in conjunction with {\sc ib1} ({\tt -a 0}). The possible values
are:

	\begin{description}

	\item n=0 -- Weighted Overlap metric (default). See
	section~\ref{overlap}. The difference between two
	feature-values is 1 if they are different and 0 if they are
	exactly the same. Can be used in combination with
	feature-weights that are specified using the {\tt -w}
	argument.

	\item n=1 -- Modified Value Difference Metric. See
	section~\ref{mvdm}. The difference between two feature-values
	is a continuous measure which depends on the difference
	between their conditional probability distribution over the
	target categories. The differences between all pairs of
	feature-values are computed before the test phase, unless the
	number of feature-values is too large, or the {\tt --} option
	is used. This metric can be used in combination with
	feature-weights that are specified using the {\tt -w} argument.

	\end{description}

\item {\tt -w <n>} : chooses between feature-weighting possibilities.
The weights are used in the metric of {\sc ib1} and in the ordering of the
{\sc igtree}. Possible values are:

	\begin{description}
	\item n=0 -- No weighting, i.e. all features have the same
	importance (weight = 1).
	\item n=1 -- Gain Ratio weighting (default). See section~\ref{infogain}.
	\item n=2 -- Information Gain weighting. See section~\ref{infogain}.
	\item n=filename -- Instead of a number we can supply a
	filename to the {\tt -w} option. This causes TiMBL to read this file
	and use its contents as weights. (See section~\ref{weightformat} for a
	description of the weights file)
	\end{description}

\item {\tt -k <n>} : Number of nearest neighbors used for
extrapolation. Only applicable in conjunction with {\sc ib1} ({\tt -a
0}) and {\sc tribl} ({\tt -a 2}). The default is 1. Especially with
the {\sc mvdm} metric it is often useful to determine a good value
larger than 1 for this parameter (usually an odd number, to avoid
ties). Note that due to ties (instances with exactly the same
similarity to the test instance) the number of instances used to
extrapolate might in fact be much larger than this parameter.

\item {\tt -q <n>} : {\tt n} is the {\sc tribl} offset, the index
number of the feature where {\sc tribl} should switch from {\sc
igtree} to {\sc ib1}. Does not apply to the other two algorithms.

\item {\tt -R <n>} : Resolve ties in the classifier randomly, using a
random generator with seed n. As a default this is OFF, and ties are
resolved in favor of the category which is more frequent in the
training set as a whole---remaining ties are resolved on a first come
first served basis.

\item {\tt -t <@file>} : If the filename given after {\tt -t} starts
with '{\tt @}', TiMBL will read commands for testing from {\tt file}.
This file should contain one set of instructions per line. On each
line new values can be set for the following command line options:
{\tt -D -e -F -k -m -o -p -q -R -t -u -v -w -x -\% --}. It is compulsory
that each line in {\tt file} contains a {\tt -t <testfile>} argument
to specify the name of the test file.

\end{description}

\section{Input options}

\begin{description}

\item {\tt -F <format>} : Force TiMBL to interpret the training and
test file as a specific data format. Possible values for this
parameter are: {\tt Compact, C4.5, ARFF, Columns}
(case-sensitive). The default is that TiMBL guesses the format from
the contents of the first line of the data file. See
section~\ref{dataformats} for description of the data formats and the
guessing rules. The {\tt Compact} format cannot be used with numeric
features.

\item {\tt -s <k,...,l-m>} : Skip features k,... and l through m. After
the {\tt -s} option a string is given with a comma-separated list of
features which will be ignored during training and
testing. Consecutive features may be indicated as a range (e.g.~{\tt
1,4,5-9,13}). The effect is the same as setting a feature's weight to
the value {\tt Ignored}. This has an advantage over setting the
weights to zero, because zero-weighted features are still present in
the learner's internal representation and can have undesirable
side-effects, especially with the {\sc igtree} algorithm.

\item {\tt -N <k,...,l-m>} : Treat features k,... and l through m as
numeric. After the {\tt -N} option a string is given with a
comma-separated list of features which will be interpreted as numeric
during training and testing. Consecutive features may be
indicated as a range (e.g.~{\tt 1,4,5-9,13}).

\item {\tt -l <n>} : Feature length. Only applicable with the Compact
data format; {\tt <n>} is the number of characters used for each
feature-value and category symbol.

\item {\tt -i <treefile>} : Skip the first two training phases, and
instead of processing a training file, read a previously saved (see
{\tt -I} option) instance-base or {\sc igtree} from the file {\tt
treefile}. See section~\ref{treeformat} for the format of this file.

\item {\tt -u <mvdmmatrixfile>} : Replace the computed {\sc mvdm} matrix
with the matrices provided in this file.

\item {\tt -P <path>} : Specify a path to read the data files
from. This path is ignored if the name of the data file already
contains path information.

\end{description}

\section{Output options}

\begin{description}

\item {\tt -I <treefile>} : After phase one and two of learning, save
the resulting tree-based representation of the instance-base or {\sc igtree}
in a file. This file can later be read back in using the {\tt -i} option
(see above). See section~\ref{treeformat} for a description of the resulting
file's format.

%\item {\tt -d} : Keep distributions. This option only has effect with
%the above {\tt -I} option, and causes the information about target
%category frequencies to be retained in the tree file. With {\sc ib1}
%this is always ON. For {\sc igtree} it is OFF; turning it ON has no
%effects on classification accuracy, but only writes the distributions
%in the tree file.

\item {\tt -W <file>} : Save the currently used feature-weights in a
file.

\item {\tt -U <mvdmmatrixfile>} : Write the computed {\sc mvdm} matrix to
this file.

\item {\tt -n <file>} : Save the feature-value and target category
symbols in a C4.5 style ``names file'' with the name {\tt
<file>}. Take caution of the fact that TiMBL does not mind creating a
file with ',' '.' '|' and ':' values in features. C4.5 will choke on this.

\item {\tt -p <n>} : Indicate progress during training and testing
after every n processed patterns. The default setting is 10000.

\item {\tt -e <n>} : During testing, compute and print an estimate on
how long it will take to classify n test patterns. This is off by
default.

\item {\tt -V} : Show the version number.

\item {\tt -v <n>} : Verbosity Level; determines how much information
is written to the output during a run. Unless indicated otherwise,
this information is written to standard error. This parameter can
take on the following values:

	\begin{description}
	\item n=0 -- Almost silent mode.
	\item n=1 -- give an overview of the settings.
	\item n=2 -- show the computed feature weights (default)
	\item n=4 -- show pairwise value difference distances.
	\item n=8 -- show exact matches.
	\item n=16 -- write the distribution that was used for extrapolation to the output file.
	\item n=32 -- write the nearest neighbors used for extrapolation to the output file. (this turns on the {\tt -x} and {\tt --} options on).
	\item Setting n to be the sum of any number of the above
values, results in combined levels of verbosity.
	\end{description}

\item {\tt -D} : Write the distance of the nearest neighbor of each
test item to the output file. In the case of the {\sc igtree}
algorithm the resulting number represents the depth of the tree at
which the classification decision was made.

\item {\tt -\%} : Write the percentage of correctly classified test
                 instances to a file with the same name as the output
                 file, but with the suffix ``{\tt .\%}''.

\item {\tt -o <suffix>} : Add {\tt suffix} to the name of the output
file. Useful for different runs with the same settings on the same
testfile.

\item {\tt -O <path>} : Write all output to the path given here. The
default is to write all output to the directory where the test file
is located.

\end{description}

\section{Internal representation options}

\begin{description}
\item {\tt -T <n>} : Order the instance-base according to one of the
	      following measures. Different measures produce different
	      tree sizes, and thus this option can be used to get
	      smaller memory usage, depending on the peculiarities
	      of the data set

	\begin{description}
	\item n=1 -- use the order of the features in the training
	file.
	\item n=2 -- use Gain Ratio to order features (default for
	{\sc igtree}).
	\item n=3 -- use Information Gain to order the features.
	\item n=4 -- order according to the quantity $\frac{1}{number
	\ of\ feature\ values}$.
	\item n=5 -- order according to the quantity $\frac{Gain
	Ratio}{number\ of\ feature\ values}$. (default for {\sc ib1})
	\item n=6 -- order according to the quantity $\frac{Information
	Gain}{number\ of\ feature\ values}$.
	\end{description}

\item {\tt -x} : Turns off the shortcut search for exact matches in
{\sc ib1}. The default is for this to be ON (which is usually much faster),
but when $k>1$, the shortcut produces different results from a
``real'' $k$ nearest neighbors search.

\item {\tt --} : Turn off the use of ``memory-for-speed''
optimizations. This option has a different effect depending on which
metric is used. With the Weighted Overlap metric ({\tt -m 0}), it
turns off the computation of inverted files. Turning this off will
make testing slower, but reduces the memory load approximately by a
half. With the {\sc mvdm} metric, ({\tt -m 1}) it turns off the
pre-computation of the value difference matrices. Turning this off
will make testing slower, but is sometimes a sheer necessity
memory-wise. With both metrics, the default is ON.

\end{description}

\chapter{Programmer's reference to the TiMBL API}
\label{programmers}

In contrast to version 1.0 of TiMBL, the whole program was set up in a
much more object-oriented manner. This makes it possible to offer you
a set of C++ classes and an Application Programming Interface (API) to
access TiMBL classifiers directly from your own programs. For example,
if you are building a Part-of-Speech tagger, you want to initialize
several classifiers once, and let them classify test items as they
come along, rather than in a batch. Or perhaps you want to use the
output of one classifier as input to the next, or you want to
customize the TiMBL interface to your needs. This is all possible. All
you have to do is to include the TiMBL header file(s) in your program:

\begin{verbatim}
#include "MBLClass.h"
\end{verbatim}

\noindent
In this software distribution you can find two example programs ({\tt
tse.cc} and {\tt classify.cc}) that demonstrate the use of TiMBL
classes in another C++ program. Have a look at these to get a feel for
how this works. To compile your own programs, or these examples, you
need to link with the {\tt Timbllib.a} library:

\begin{verbatim}
g++ -o yourprogram yourprogram.o Timbllib.a
\end{verbatim}

\noindent
Note, however, that {\em the license does not permit you to redistribute}
modified versions of TiMBL or derivative works without prior
permission. Also, note that the API is still ``work in progress'' and
we might make changes to it in future releases.

This chapter gives an overview of the TiMBL classes needed to include
Memory-Based classifiers in your own programs, and describes the
interface through which they can be accessed. We have not attempted to
document any of the internal structure of the TiMBL source code here.

\section{Class hierarchy}
\label{classhierarchsection}

The main classes that make up TiMBL are shown in Figure~\ref{classes}.
A separate class is defined for each type of algorithm ({\tt -a 0/1/2}
in terms of command line options). The actual classifiers ({\tt
IB1Class}, {\tt IGTREEClass}, and {\tt TRIBLClass}) are derived
directly from the virtual base class {\tt MBLClass}.\footnote{There is
also a {\tt TimblExperiment}, which puts the wrappers around {\tt
MBLClass} that make up the TiMBL program. This class is not documented
here as we do not consider it clean enough to be accessible from the
outside.}

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=\columnwidth
                \epsffile{classes.eps}
                \caption{The main components of the TiMBL class hierarchy. 
		The arrows denote inheritance.
                }
                \label{classes}
        \end{center}
\end{figure}

Since {\tt MBLClass} is a virtual base class, and hence it does not
have its own constructor, you will typically want to declare an
pointer to an object of this class.

\begin{verbatim}
MBLClass * TheClassifierPtr = NULL;	
\end{verbatim}

\noindent
Such a pointer can then be instantiated to one of the existing
classes, depending on your needs.

\begin{verbatim}
switch(algorithm){
   case MBL:
     TheClassifierPtr = new IB1Class;
     break;
   case IGTREE:
     TheClassifierPtr = new IGTREEClass;
     break;
   case TRIBL:
     TheClassifierPtr = new TRIBLClass;
     break;
}	
\end{verbatim}

\noindent
Or you can directly make an object of the desired class, e.g.:

\begin{verbatim}
IGTREEClass TheClassifier;
\end{verbatim}

\section{Public member functions}
\label{membersection}

The constructors and destructors are specific for each of the derived
classes. The rest of the interface is the same for all classes and is
defined at the level of the {\tt MBLClass}. The return type of most
functions is {\tt bool}. A return value of 0 means that something went
wrong, 1 is a successful return.

\subsection{The constructors}

\begin{verbatim}
IB1Class::IB1Class( char * = NULL );
IGTREEClass::IGTREEClass( char * = NULL );
TRIBLClass::TRIBLClass( char * = NULL );
\end{verbatim}

These constructor functions take an optional {\tt char *} argument. By
way of this argument you can assign a meaningful name to the created
object. The function:

\begin{verbatim}
char * MBLClass::ExpName();
\end{verbatim}

\noindent
will return that name to you at a later point in time.

\subsection{The destructors}

\begin{verbatim}
IB1Class::~IB1Class();
IGTREEClass::~IGTREEClass();
TRIBLClass::~TRIBLClass();
\end{verbatim}

\noindent
They clean up all the mess.

\subsection{Member functions}

The first two functions below emulate the functionality of the first
two phases of TiMBL (Examining and Learning). They will act with
default values of all parameters for the type of classifier that they
are invoked from. To use different parameter settings, use the {\tt
SetOption()} function described below. After invoking {\tt
Learn(filename)}, a classifier is available for subsequent
classification of test patterns. There are several classification
functions available and several additional input/output possibilities.

\begin{description}

\item {\tt bool MBLClass::PrepareExperiment( char * $f$ );}\\ 
Use the file with name $f$ to go through phase 1 of learning (fills
the statistics tables for feature values, targets, weights etc.)  If
needed, this function is automatically called by {\tt Learn()} (see
below).

\item {\tt bool MBLClass::Learn( char * $f$ = NULL );}\\ 
Build an instance-base from the file $f$ (phase 2 of learning), and if
needed, call {\tt PrepareExperiment()} first. If $f$ {\tt == NULL}, or
the argument is omitted, then the same file is used as was used by
{\tt PrepareExperiment()} (if any).

%\item {\tt bool MBLClass::Test( char * $f$ , char * $o$, char * $p$ =
%NULL ) = 0;}\\ 
%Open the file $f$ and classify all instances from that file with the
%current classifier. The results are written to a newly created file
%$o$. If $p$ is unequal to {\tt NULL}, make a file $p$ to which the
%total percentage correct is written. Corresponds to phase 3 of TiMBL).

\item {\tt char * MBLClass::Classify\_BestString( char * $Line$);}
\item {\tt char * MBLClass::Classify\_BestString( char * $Line$, double \& distance);}\\
The string $Line$ is parsed into features according to the current
input format and classified by the present classifier. If
classification fails the function returns {\tt NULL}. Otherwise the
value that is returned will be a pointer to a string that holds the
value of the most likely category. If the variant with the extra {\tt
distance} parameter is used, the distance of the nearest neighbor used
for classification will be found in the argument variable after return.
To get the whole distribution of categories in the nearest neighbor
set, use:
\item {\tt char * MBLClass::Classify\_DistString( char * $Line$ );}
\item {\tt char * MBLClass::Classify\_DistString( char * $Line$,
double \& distance);}\\
The string that is returned contains all categories in the
distribution, together with their frequency, between curly brackets, 
e.g.~``{\tt\{ A 12 B 3 C
81\}}''. You can parse this string and extract the category values from
it yourself, so that you can e.g.~re-normalize them to form a
(conditional) probability distribution. Again, if the variant with the
extra {\tt distance} parameter is used, the distance of the nearest
neighbor used for classification will be found in the argument
variable after return.
\noindent
With both {\tt Classify\_BestString()} and {\tt Classify\_DistString()}
you are responsible to {\tt delete []} the resulting pointer when you
have no more use for it.

\item {\tt bool MBLClass::GetInstanceBase( const char * $f$ );}\\
Read an instance base from file $f$.

\item {\tt bool MBLClass::WriteInstanceBase( const char * $f$ );}\\
Write an instance base to file $f$.

\item {\tt bool MBLClass::GetWeights( const char * $f$ );}\\ Read the
weights to be used from file $f$.

\item {\tt bool MBLClass::SaveWeights( const char * $f$ );}\\
Write the currently used weights to file $f$.

%\item {\tt bool MBLClass::ShowWeights( ostream\& $os$ );}\\
%Write the weights to the specified {\tt ostream}.
%
\item {\tt bool MBLClass::GetArrays( const char * $f$ );}\\
Read the {\sc mvdm} matrices from file $f$.

\item {\tt bool MBLClass::WriteArrays( const char * $f$ );}\\
Write the {\sc mvdm} matrices to file $f$.

\item {\tt bool MBLClass::WriteNamesFile( const char * $f$ );}\\
Create a C4.5 namesfile $f$.

\end{description}

\subsection{Setting parameters and options}

Virtually all of the parameters and options that can be specified in
TiMBL using command line options, can also be controlled in the
classes derived from {\tt MBLClass}. This is done with the {\tt
SetOption()} function:

\begin{description}
\item {\tt bool MBLClass::SetOption( const char * $s$ );}\\ 
The string that is given as an argument contains a option setting
command of the form:\\
{\tt ``option : value''}\\
Case does not matter, so you can achieve the same result by calling\\
\noindent
{\tt SetOption( "NEIGHBORS: 2" );}\\ 
or\\
\noindent
{\tt SetOption( "neighBors: 2" );}\\ 
The possible option setting commands, together with their permitted
values are given in the table below. Many combinations or sequences of
settings are not permissible, because they ``don't make sense''.  The
caller of the function is responsible for rule out impossible
combinations. The {\tt SetOption} function will return 0 if an attempt
is made to set a non-existing option or to set an existing option to a
non-existing value.

\item The following functions are used to display option information:

\item {\tt bool MBLCLass::ShowOptions(void);}\\
Shows all options with their current and possible values.

\item {\tt bool MBLCLass::ShowSettings(void);}\\
Shows only the current settings of all options.

\end{description}

\noindent
An overview of the options and their values:\\[2ex]
\begin{tabular}{|l|p{12cm}|}
\hline
Option & Values\\
\hline
{\tt METRIC} &  {\tt Overlap}\\
	     &  {\tt PVD} or {\tt Prestored Value Difference} (This is
the same as if you set {\tt -m 1} on the command line and {\em don't}
set {\tt --}. i.e. the value difference matrix is pre-computed)\\
	     &  {\tt VD} or {\tt Value Difference} (no pre-computation)\\
	     & select the metric (only applies to {\sc ib1} and {\sc tribl} algorithms).\\
	     & example: {\tt SetOption( "Metric: VD" );}\\
\hline
{\tt NEIGHBORS} & $n$ (integer)\\
		 & sets the number of neighbors to use (does not apply
for {\sc igtree} classifiers.\\
		 & example: {\tt SetOption( "NEIGHBORS: 2" );}\\
\hline
{\tt WEIGHTING}  & {\tt NOW} or {\tt No Weighting}\\
                 &  {\tt GRW} or {\tt GainRatio}\\
                 &  {\tt IGW} or {\tt InfoGain}\\
		 & selects the desired feature weighting scheme.\\
		 & example: {\tt SetOption( "Weighting: InfoGain" );}\\
\hline
\end{tabular}

\begin{tabular}{|l|p{12cm}|}
\hline
Option & Values\\
\hline
{\tt TO\_IGNORE}	 & $n$ (integer)\\
		 & causes the feature with index $n$ to be ignored.\\
		 & example: {\tt SetOption( "To\_ignore:55" );}\\
\hline
{\tt NUMERIC}	 & $n$ (integer)\\
		 & causes the feature with index $n$ to be interpreted as numerical\\
		 & example: {\tt SetOption( "Numeric:3" );}\\
\hline
{\tt TRIBL\_OFFSET} & $n$ (integer)\\
		 & only applies to the {\sc tribl} algorithm. Sets the
feature at which a transition is made from {\sc igtree} to {\sc tribl}.\\
		 & example: {\tt SetOption( "Tribl\_offset: 3" );}\\
\hline
{\tt TREE\_ORDER} & {\tt UDO} or {\tt Data File Ordering}\\ 
                 & {\tt DO}  or {\tt Default Ordering}\\ 
                 & {\tt GRO} or {\tt GainRatio}\\ 
                 & {\tt IGO} or {\tt InformationGain}\\ 
                 & {\tt 1/V} or {\tt Inverse Values}\\ 
                 & {\tt G/V} or {\tt GainRatio/Values}\\ 
                 & {\tt I/V} or {\tt InformationGain/Values}\\ 
                 & {\tt 1/S} or {\tt Inverse SplitInfo}\\  
		 & Changes the ordering of the features in the instance base tree.\\
		 & example: {\tt SetOption( "TREE\_ORDER: DO" );}\\
\hline
{\tt INPUTFORMAT} & {\tt Compact}\\
                  & {\tt C45} or {\tt C 4.5}\\
                  & {\tt Columns}\\
                  & {\tt ARFF}\\
		  & Forces input to be interpreted in this format.\\
		  & example: {\tt SetOption( "InputFormat : ARFF" );}\\
\hline
{\tt FLENGTH}     & $n$ (integer)\\
		  & If {\tt INPUTFORMAT} is {\tt Compact}, you have to specify how wide the feature values are ($n$ number of characters).\\
		  & example: {\tt SetOption( "FLENGTH: 2" );}\\
\hline
{\tt SEED}        & $n$ (integer)\\
		  & sets the seed $n$ for the random generator.\\
		  & example: {\tt SetOption( "SEED: 123" );}\\
\hline
{\tt VERBOSITY}	  & {\tt 0} -- output just the minimal amount of information.\\
		  & {\tt 1} -- give an overview of the settings.\\
		  & {\tt 2} -- show the computed feature weights (this is the
default)\\
		  & {\tt 4} -- show value difference matrices.\\
		  & {\tt 8} -- show each exact matches.\\
		  & {\tt 16} -- write the distribution that was used for extrapolation to the output file.\\
		  & {\tt 32} -- write the nearest neighbors used for extrapolation to the output file.\\
		  & Setting the sum of any number of the above
values, results in combined levels of verbosity.\\
		  & example: {\tt SetOption( "VERBOSITY: 56" );}\\
\hline
\end{tabular}

\begin{tabular}{|l|p{12cm}|}
\hline
Option & Values\\
\hline
{\tt EXACT\_MATCH} & {\tt true} : prefer exact matches during testing.\\
		  & {\tt false} : return all neighbors regardless of exact matches.\\
		  & example: {\tt SetOption( "exact\_match: true" );}\\
\hline
{\tt USE\_INVERTED}& {\tt true} : use inverted files when testing.\\
		  & {\tt false} : don't.\\
		  & example: {\tt SetOption(  "use\_inverted: 1" );}\\
\hline
%{\tt SHOW\_DISTANCES}& {\tt true} : output the distance of the nearest neighbors to the output file when testing.\\
%		  & {\tt false} : don't.\\
%		  & example: {\tt SetOption(  "Show\_Distances: true" );}\\
%\hline
%{\tt ESTIMATE}    & $n$ (integer)\\
%		  & makes an estimate of the end-time of the experiment after $n$ lines of test examples.\\
%		  & example: {\tt SetOption( "ESTIMATE: 10000" );}\\
%\hline
{\tt PROGRESS}    & $n$ (integer)\\
		  & indicates how often (number of lines) you want to 
		    see an update on the experiment's progress.\\
		  & example: {\tt SetOption( "Progress: 10000" );}\\
\hline
\end{tabular}

\bibliographystyle{plain}
\bibliography{../wonderland/wonderland}

\appendix

\chapter{Tutorial: a case study}
\label{tutorial}

This tutorial is meant to get you started with TiMBL quickly. We
discuss how to format the data of a task to serve as training
examples, which choices can be made during the construction of the
classifier, how various choices can be evaluated in terms of their
generalization accuracy, and various other practical issues. The
reader who is interested in more background information on TiMBL
implementation issues and a formal description of Memory-Based
Learning, is advised to read Chapter~\ref{algorithms}.

Memory-Based Learning ({\sc mbl}) is based on the idea that
intelligent behavior can be obtained by analogical reasoning, rather
than by the application of abstract {\em mental rules} as in rule
induction and rule-based processing. In particular, {\sc mbl} is
founded in the hypothesis that the extrapolation of behavior from
stored representations of earlier experience to new situations, based
on the similarity of the old and the new situation, is of key
importance.

{\sc mbl} algorithms take a set of examples (fixed-length patterns of
feature-values and their associated class) as input, and produce a
{\em classifier} which can classify new, previously unseen, input
patterns. Although TiMBL was designed with linguistic classification
tasks in mind, it can in principle be applied to any kind of
classification task with symbolic or numeric features and discrete
(non-continuous) classes for which training data is available. As an
example task for this tutorial we go through the application of TiMBL
to the prediction of Dutch diminutive suffixes. The necessary data
sets are included in the TiMBL distribution, so you can replicate the
examples given below on your own system.

\section{Data}

The operation of TiMBL will be illustrated below by means of a real
natural language processing task: prediction of the diminutive suffix
form in Dutch~\cite{Daelemans+97e}. In Dutch, a noun can receive a
diminutive suffix to indicate {\em small size} literally or
metaphorically attributed to the referent of the noun; e.g. {\em
mannetje} means {\em little man}. Diminutives are formed by a
productive morphological rule which attaches a form of the Germanic
suffix {\em -tje} to the singular base form of a noun. The suffix
shows variation in its form (Table \ref{variation}). The task we
consider here is to predict which suffix form is chosen for previously
unseen nouns on the basis of their form.

\begin{table}[h]
\begin{center}
\begin{tabular}{l|l|l}
Noun & Form & Suffix \\
\hline
huis (house) & huisje & {\em -je} \\
man (man) & mannetje & {\em -etje\/} \\
raam (window) & raampje & {\em -pje\/} \\
woning (house) & woninkje & {\em -kje\/} \\
baan (job) & baantje & {\em -tje\/} \\
\end{tabular}
\caption{Allomorphic variation in Dutch diminutives.}\label{variation}
\end{center}
\end{table}

For these experiments, we collect a representation of nouns in terms
of their syllable structure as training material\footnote{These words
were collected form the {\sc celex} lexical
database~\cite{Baayen+93}}. For each of the last three syllables of
the noun, four different features are collected: whether the syllable
is stressed or not (values - or +), the string of consonants before
the vocalic part of the syllable (i.e. its onset), its vocalic part
(nucleus), and its post-vocalic part (coda). Whenever a feature value
is not present (e.g. a syllable does not have an onset, or the noun
has less than three syllables), the value `=' is used. The class to be
predicted is either E (-etje), T (-tje), J (-je), K (-kje), or P
(-pje).

Some examples are given below (the word itself is only provided for
convenience and is not used). The values of the syllabic content
features are given in phonetic notation.

\begin{table}[h]
\begin{tabular}{|cccccccccccc|l|l|l|}
\hline
- & b & i & = & - & z & @ & = & + & m & A & nt & J & biezenmand \\
= & = & = & = & = & = & = & = & + & b & I & x & E & big\\
= & = & = & = & + & b & K & = & - & b & a & n & T & bijbaan\\
= & = & = & = & + & b & K & = & - & b & @ & l & T & bijbel\\
\hline
\end{tabular}
\end{table}

Our goal is to use TiMBL in order to train a classifier that can
predict the class of new, previously unseen words as correctly as
possible, given a set of training examples that are described by the
features given above. Because the basis of classification in TiMBL is
the storage of all training examples in memory, a test of the
classifier's accuracy must be done on a separate test set. We will
call these datasets {\tt dimin.train} and {\tt dimin.test},
respectively. The training set {\tt dimin.train} contains 2999 words
and the test set contains 950 words, none of which are present in the
training set. Although a single train/test partition suffices here for
the purposes of explanation, it does not factor out the bias of
choosing this particular split. Unless the test set is sufficiently
large, a more reliable generalization accuracy measurement is used in
real experiments, e.g.~10-fold cross-validation~\cite{Weiss+91}. This
means that 10 separate experiments are performed, and in each ``fold''
90\% of the data is used for training and 10\% for testing, in such a
way that each instance is used as a test item exactly once.

\section{Using TiMBL}

Different formats are allowed for training and test data files. TiMBL
is able to guess the type of format in most cases. We will use
comma-separated values here, with the class as the last value. This
format is called C4.5 format in TiMBL because it is the same as that
used in Quinlan's well-known C4.5 program for induction of decision
trees~\cite{Quinlan93}. See Section~\ref{fileformats} for more
information about this and other file formats.

An experiment is started by executing TiMBL with the two files ({\tt
dimin.train} and {\tt dimin.test}) as arguments:

{\small
\begin{verbatim}
Timbl -f dimin.train -t dimin.test
\end{verbatim}
}

Upon completion, a new file has been created with name\\ {\tt
dimin.test.mbl.wo.gr.k1.out}, which is in essence identical to the
input test file, except that an extra comma-separated column is added
with the class predicted by TiMBL. The name of the file provides
information about the {\sc mbl} algorithms and metrics used in the
experiment (the default values in this case). We will describe these
shortly.

Apart from the result file, information about the operation of the
algorithm is also sent to the standard output. It is therefore 
advisable to redirect the output to a file in order to make a log of
the results.

{\small
\begin{verbatim}
Timbl -f dimin.train -t dimin.test > dimin-exp1
\end{verbatim}
}

The defaults used in this case work reasonably well for most problems.  We
will now provide a point by point explanation of what goes on in the
output.\\

\vspace{1cm}

\hline

{\small
\begin{verbatim}
TiMBL Version 2.0 (c) ILK 1998,1999.
Tilburg Memory Based Learner
Induction of Linguistic Knowledge Research Group, Tilburg University 
Tue Dec 15 14:36:02 1998

Examine datafile gave the following results:
Number of Features: 12
InputFormat       : C4.5
\end{verbatim}
}

\hline
\vspace{1cm}

TiMBL has detected 12 features and the C4.5 input format
(comma-separated features, class at the end).

\vspace{2cm}
\hline

{\small
\begin{verbatim}
Phase 1: Reading Datafile: dimin.train
Start:          0 @ Tue Dec 15 14:36:02 1998
Finished:    2999 @ Tue Dec 15 14:36:02 1998

Calculating Entropy       Tue Dec 15 14:36:02 1998
Lines of data     : 2999
DB Entropy        : 1.6177063
Number of Classes : 5

Feature  Values         SplitInfo       InfoGain        GainRatio
    1         3         1.2440184       0.030971070     0.024895990
    2        50         2.2083279       0.060848303     0.027554017
    3        19         2.1177766       0.039559999     0.018679968
    4        37         0.99822179      0.052525782     0.052619350
    5         3         1.5624980       0.074259788     0.047526326
    6        61         4.3324680       0.10583338      0.024427965
    7        20         3.5323986       0.12320692      0.034879110
    8        69         2.2093120       0.097205759     0.043998204
    9         2         0.97736734      0.045678211     0.046735970
   10        64         4.9917346       0.21385674      0.042842170
   11        18         3.6189077       0.66964325      0.18504016
   12        43         3.9277729       1.2779932       0.32537350

Feature Permutation based on GainRatio/Values :
< 9, 5, 11, 1, 12, 7, 4, 3, 10, 8, 2, 6 >
\end{verbatim}
}

\hline
\vspace{1cm}

Phase 1 is the training data analysis phase. Time stamps for start and
end of analysis are provided. Some preliminary analysis of the
training data is done: number of training items, number of classes,
entropy of the training data. For each feature, the number of values,
and three variants of an information-theoretic measure of feature
relevance are given. These are used both for memory organization
during training and for feature relevance weighting during testing
(see Chapter~\ref{algorithms}). Finally, an ordering (permutation) of
the features is given. This ordering is used for building the
tree-index to the case-base.

\vspace{1cm}
\hline

{\small
\begin{verbatim}
Phase 2: Learning from Datafile: dimin.train
Start:          0 @ Tue Dec 15 14:36:02 1998
Finished:    2999 @ Tue Dec 15 14:36:03 1998

Size of InstanceBase = 19236 Nodes, (384720 bytes)
\end{verbatim}
}

\hline
\vspace{1cm}

Phase 2 is the learning phase; all training items are stored in an
efficient way in memory for use during testing. Again timing
information (real time) is provided, as well as information about the
size of the data structure representing the stored examples.

\vspace{1cm}
\hline

{\small
\begin{verbatim}
Starting to test, Testfile: dimin.test
Writing output in:          dimin.test.mbl.wo.gr.k1.out
Algorithm   : IB1
Test metric : Overlap (Using Inverted files, prefering exact matches)
Weighting   : GainRatio

Tested:      1 @ Tue Dec 15 14:36:03 1998
Tested:      2 @ Tue Dec 15 14:36:03 1998
Tested:      3 @ Tue Dec 15 14:36:03 1998
Tested:      4 @ Tue Dec 15 14:36:03 1998
Tested:      5 @ Tue Dec 15 14:36:03 1998
Tested:      6 @ Tue Dec 15 14:36:03 1998
Tested:      7 @ Tue Dec 15 14:36:03 1998
Tested:      8 @ Tue Dec 15 14:36:03 1998
Tested:      9 @ Tue Dec 15 14:36:04 1998
Tested:     10 @ Tue Dec 15 14:36:04 1998
Ready:     950 @ Tue Dec 15 14:36:25 1998
Seconds taken: 22 (43.18 p/s)
918/950 (0.966316), of which 39 exact matches 
\end{verbatim}
}

\hline
\vspace{1cm}

In Phase 3, the trained classifier is applied to the test set. Because
we have not specified which algorithm to use, the default settings are
used ({\sc ib1} with information theoretic feature weighting). This
algorithm computes the similarity between a test item and each
training item in terms of {\em weighted overlap}: the total difference
between two patterns is the sum of the relevance weights of those
features which are not equal. The class for the test item is decided
on the basis of the least distant item(s) in memory. To compute
relevance, Gain Ratio is used (an information-theoretic measure, see
Section~\ref{infogain}). Time stamps indicate the progress of the
testing phase. Finally, accuracy on the test set is logged, and the
number of exact matches\footnote{An exact match in this experiment can
occur when two different nouns have the same feature-value
representation.}. In this experiment, the diminutive suffix form of
96.6\% of the new words was correctly predicted.

The meaning of the output file names can be explained now:\\ {\tt
dimin.test.mbl.wo.gr.k1.out} means output file ({\tt .out}) for {\tt
dimin.test} with algorithm {\sc mbl} (={\sc ib1}), similarity computed as {\em
weighted overlap} ({\tt .wo}), relevance weights computed with {\em
gain ratio} ({\tt .gr}), and number of most similar memory patterns on
which the output class was based equal to 1 ({\tt .k1}).

\section{Algorithms and Metrics}

A precise discussion of the different algorithms and metrics
implemented in TiMBL is given in Chapter~\ref{algorithms}. We will
discuss the effect of the most important ones on our data set.

A first choice in algorithms is between using {\sc ib1} and {\sc
igtree}. In the trade-off between generalization accuracy and
efficiency, {\sc ib1} usually, but not always, leads to more accuracy
at the cost of more memory and slower computation, whereas {\sc
igtree} is a fast heuristic approximation of {\sc ib1}, but sometimes
less accurate. The {\sc igtree} algorithm is used when {\tt -a 1} is
given on the command line, whereas the {\sc ib1} algorithm used above
(the default) would have been specified explicitly by {\tt -a 0}.

{\small
\begin{verbatim}
Timbl -a 1 -f dimin.train -t dimin.test
\end{verbatim}
}

When using the {\sc ib1} algorithm, there is a choice of metrics for
influencing the definition of similarity. With {\em weighted overlap},
each feature is assigned a weight, determining its relevance in
solving the task. With the {\em modified value difference metric}
({\sc mvdm}), each pair of values of a particular feature is assigned
a value difference. The intuition here is that in our diminutive
problem, for example, the codas $n$ and $m$ should be regarded as
being more similar than $n$ and $p$. These pair-wise differences are
computed for each pair of values in each feature (see
Section~\ref{mvdm}). Selection between weighted overlap and {\sc mvdm}
is done by means of the {\tt -m} parameter. The following selects {\sc
mvdm}, whereas {\tt -m 0} ({\em weighted overlap}) is the default.

{\small
\begin{verbatim}
Timbl -m 1 -f dimin.train -t dimin.test
\end{verbatim}
}

Especially when using {\sc mvdm}, but also in other cases, it may be
useful to extrapolate not just from the most similar example in
memory, which is the default, but from several. This can be achieved
by using the $-k$ parameter followed by the wanted number of nearest
neighbors. E.g., the following applies {\sc ib1} with the {\sc mvdm}
metric, with extrapolation from the 5 nearest neighbors.

{\small
\begin{verbatim}
Timbl -m 1 -k 5 -f dimin.train -t dimin.test
\end{verbatim}
}

Within the {\sc ib1} {\em weighted overlap} option, the default
feature weighting method is Gain Ratio. By setting the parameter {\tt
-w} to 0, an {\em overlap} definition of similarity is created where
each feature is considered equally relevant. Similarity reduces in
that case to the number of equal values in the same position in the
two patterns being compared. As an alternative weighting, users can
provide their own weights by using the {\tt -w} parameter with a
filename in which the feature weights are stored (see
Section~\ref{weightformat} for a description of the format of the
weights file).

\begin{table}
\begin{tabular}{|l|r|r|r|r|}\hline
             & gain ratio & inform. gain & overlap & {\sc mvdm} \\
\hline
{\sc igtree} & 96.4 & 96.4 &  & \\
{\sc ib1}, $-k1$ & 96.6 & 96.5 & 84.9 & 96.2 \\
{\sc ib1}, $-k10$ & & & & 97.8 \\
\hline
\end{tabular}
\caption{Some results for diminutive prediction.}
\label{diminresults}
\end{table}

Table \ref{diminresults} shows the effect of algorithm, metric, and
weighting method choice on generalization accuracy for our training -
test set partition. We see that {\sc igtree} performs slightly worse
than {\sc ib1} for this task (it uses less memory and is faster,
however). When comparing {\sc mvdm} and {\em feature weighting}, we
see that the overall best results are achieved with {\sc mvdm}, but
only with a relatively high value for $k$, the number of memory items
on which the extrapolation is based. Increasing the value of $k$ for
(weighted) Overlap metrics decreased performance. Within the feature
weighting approaches, overlap (i.e. no weighting) performs markedly
worse than the default {\em information gain} or {\em gain ratio}
weighting methods.

\section{More Options}

Several input and output options exist to make life easier while
experimenting. See Chapter~\ref{commandline} for a detailed
description of these options. One especially useful option for testing
linguistic hypotheses is the {\tt -s} command line option, which
allows you to skip certain features when computing similarity. E.g. if
we want to test the hypothesis that only the rime (nucleus and coda)
and the stress of the last syllable are actually relevant in
determining the form of the diminutive suffix, we can execute the
following to disregard all but the fourth-last and the last two
features. As a result we get an accuracy of 97.8\%\footnote{It should
be kept in mind that the amount of overlap in training and test set
has significantly increased, so that generalization is based on
retrieval more than on similarity computation.}.

{\small
\begin{verbatim}
Timbl -s 1-8,10 -f dimin.train -t dimin.test
\end{verbatim}
}

Another useful parameter we discuss here is the {\tt -D} command line
option which has as effect that in the output file not only the
extrapolated class is appended to the input pattern, but also the
distance to the nearest neighbor.

{\small
\begin{verbatim}
Timbl -D -f dimin.train -t dimin.test
\end{verbatim}
}

The resulting output file contains lines like the following.

{\small
\begin{verbatim}
-,t,@,=,-,l,|,=,-,G,@,n,T,T        0.099723
-,=,I,n,-,str,y,=,+,m,E,nt,J,J        0.123322
=,=,=,=,=,=,=,=,+,br,L,t,J,J        0.042845
=,=,=,=,+,zw,A,=,-,m,@,r,T,T        0.059425
=,=,=,=,-,f,u,=,+,dr,a,l,T,T        0.077798
=,=,=,=,=,=,=,=,+,l,e,w,T,T        0.042845
=,=,=,=,+,tr,K,N,-,k,a,rt,J,J        0.068456
=,=,=,=,+,=,o,=,-,p,u,=,T,T        0.172314
=,=,=,=,=,=,=,=,+,l,A,m,E,E        0.042845
=,=,=,=,=,=,=,=,+,l,A,p,J,J        0.042845
=,=,=,=,=,=,=,=,+,sx,E,lm,P,P        0.042845
+,l,a,=,-,d,@,=,-,k,A,st,J,J        0.070701
\end{verbatim}
}

This can be used to study how specific instances (low distance) and
more general patterns (higher distance) are used in the process of
generalization.\\

The {\sc -v} (verbosity) option allows you to control the amount of
information that is generated in the output, ranging from nothing much
({\sc -v 0}) to a lot ({\sc -v 63}). Specific (combinable) verbosity
levels exist for dumping parameter settings, feature relevance
weights, value difference matrices, exact matches, distributions instead
of discrete classes, and the nearest neighbors on which decision are
based. E.g. the following command results in an output file with
distributions. 

{\small
\begin{verbatim}
Timbl -v 16 -s 1-8,10 -f dimin.train -t dimin.test
\end{verbatim}
}

{\small
\begin{verbatim}
=,=,=,=,=,=,=,=,+,p,e,=,T,T { T 51 }
=,=,=,=,+,k,u,=,-,bl,u,m,E,E { E 4 P 3 }
+,m,I,=,-,d,A,G,-,d,},t,J,J { J 1 }
=,=,=,=,=,=,=,=,+,l,A,m,E,E { E 12 }
=,=,=,=,=,=,=,=,+,l,A,p,J,J { E 4 J 9 }
=,=,=,=,=,=,=,=,+,sx,E,lm,P,P { P 1 }
=,=,=,=,=,=,=,=,+,sp,A,N,E,E { E 8 }
+,k,a,=,-,k,@,=,-,n,E,st,J,J { J 1 }
=,=,=,=,+,v,u,=,-,r,I,N,K,K { E 34 K 61 }
=,=,=,=,=,=,=,=,+,v,A,t,J,J { T 1 J 17 }
-,r,@,=,+,G,I,s,-,t,@,r,T,T { T 296 }
\end{verbatim}
}

This information can e.g. be used to assign a certainty to a decision
of the classifier, or to make available a second-best back-off option.

The listing of nearest neighbors is useful for the analysis of the
behavior of a classifier.

{\small
\begin{verbatim}
Timbl -v 32 -s 1-8,10 -f dimin.train -t dimin.test
\end{verbatim}
}

{\small
\begin{verbatim}
+,m,I,=,-,d,A,G,-,d,},t,J,J
# k=1, 1 Neighbor(s) at distance: 0.282544
#       -,v,@,r,+,v,A,l,-,p,},t, -*-
-,t,@,=,-,l,|,=,-,G,@,n,T,T
# k=1, 1 Neighbor(s) at distance: 0.0995083
#       -,x,@,=,+,h,|,=,-,G,@,n, -*-
-,=,I,n,-,str,y,=,+,m,E,nt,J,J
# k=1, 1 Neighbor(s) at distance: 0.123281
#       -,m,o,=,-,n,y,=,+,m,E,nt, -*-
=,=,=,=,=,=,=,=,+,br,L,t,J,J
# k=1, 4 Neighbor(s) at distance: 0.0428422
#       =,=,=,=,=,=,=,=,+,r,L,t, -*-
#       =,=,=,=,=,=,=,=,+,kr,L,t, -*-
#       =,=,=,=,=,=,=,=,+,sx,L,t, -*-
#       =,=,=,=,=,=,=,=,+,fl,L,t, -*-
=,=,=,=,+,zw,A,=,-,m,@,r,T,T
# k=1, 5 Neighbor(s) at distance: 0.0593071
#       =,=,=,=,+,fl,e,=,-,m,@,r, -*-
#       =,=,=,=,+,=,E,=,-,m,@,r, -*-
#       =,=,=,=,+,l,E,=,-,m,@,r, -*-
#       =,=,=,=,+,k,a,=,-,m,@,r, -*-
#       =,=,=,=,+,h,O,=,-,m,@,r, -*-
\end{verbatim}
}

We hope that this tutorial has made it clear that, once you have coded
your data in fixed-length feature-value patterns, it should be
relatively straightforward to get the first results using TiMBL. You
can then experiment with different metrics and algorithms to try and
further improve your results.

\end{document}
