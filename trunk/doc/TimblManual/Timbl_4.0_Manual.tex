% TiMBL 4.1 manual
% slight adapations of the 4.0 version. References to 4.0 are
% changed to 4.1.

\documentclass{report}

\usepackage{epsf}
\usepackage{a4wide}
\usepackage{palatino}
\usepackage{fullname}

\newcommand{\chisq}{{$ \chi^2 $}}

\author{Walter Daelemans* \and Jakub Zavrel* \and Ko van der Sloot \and
	Antal van den Bosch\\ \ \\
	Induction of Linguistic Knowledge\\
	Computational Linguistics\\ 
        Tilburg University \\ \\
	(*) CNTS - Language Technology Group\\
	University of Antwerp\\ \\
        P.O. Box 90153, NL-5000 LE, Tilburg, The Netherlands \\ 
        URL: http://ilk.kub.nl\thanks{This document is available from
	http://ilk.kub.nl/downloads/pub/papers/ilk0104.ps.gz. All rights reserved
	Induction of Linguistic Knowledge, Tilburg University and 
        CNTS Research Group, University of Antwerp.}}

\title{{\huge TiMBL: Tilburg Memory-Based Learner} \\ \vspace*{0.5cm}
{\bf version 4.1} \\ \vspace*{0.5cm}{\huge Reference Guide}\\
\vspace*{1cm} {\it ILK Technical Report -- ILK 01-04}}

%better paragraph indentation
\parindent 0pt
\parskip 9pt

\begin{document}

\pagenumbering{roman} 

\maketitle

\tableofcontents

\chapter*{Preface}

Memory-Based Learning ({\sc mbl}) has proven to be successful in a
large number of tasks in Natural Language Processing (NLP). In our
research group at Tilburg University, we have been working since the
end of the 1980's on the development of Memory-Based Learning
techniques and algorithms\footnote{Section~\ref{furtherreading}
provides a historical overview of work on the application of {\sc
mbl} in {\sc nlp}.}. With the establishment of the ILK (Induction of
Linguistic Knowledge) research group in 1997, and increasing use of
{\sc mbl} at the CNTS research group of the University of Antwerp, the
need for a well-coded and uniform tool for our main algorithms became
more urgent. TiMBL is the result of combining ideas from a number of
different {\sc mbl} implementations, cleaning up the interface, and
using a whole bag of tricks to make it more efficient. We think it has
become a useful tool for NLP research, and, for that matter, for many
other domains where classification tasks are learned from examples.

Memory-Based Learning is a direct descendant of the classical
$k$-Nearest Neighbor ($k$-NN) approach to classification. In typical
NLP learning tasks, however, the focus is on discrete data, very large
numbers of examples, and many attributes of differing
relevance. Moreover, classification speed is a critical issue in any
realistic application of Memory-Based Learning. These constraints,
which are different from those of traditional pattern
recognition applications with their numeric features, often lead to
different data-structures and different speedup optimizations for the
algorithms. Our approach has resulted in an architecture which makes
extensive use of indexes into the instance memory, rather than the
typical flat file organization found in straightforward $k$-NN
implementations. In some cases the internal organization of the memory
results in algorithms which are quite different from $k$-NN, as is the
case with {\sc igtree}. We believe that our optimizations make TiMBL
one of the fastest discrete $k$-NN implementations around.

The main effort in the development and maintenance of this software
was invested by Ko van der Sloot. The code started as a rewrite of
{\tt nibl}, a piece of software developed by Peter Berck from a Common
Lisp implementation by Walter Daelemans of {\sc ib1-ig}. Some of the
index-optimizations in TiMBL are due to Jakub Zavrel. The code has
benefited substantially from trial, error and scrutiny by all past and
present members of the ILK and {\sc cnts} groups in Tilburg and
Antwerp. We would furthermore like to thank Ton Weijters of Eindhoven
Technical University for his inspirational early work on $k$-NN and
for his involvements in {\sc igtree}. This software was written in
the context of the ``Induction of Linguistic Knowledge'' research
programme, partially funded by the Netherlands Organization for
Scientific Research (NWO). Last but not least, our thanks go to those
users of TiMBL who have contributed to it immensely by giving us
feedback and reporting bugs.

The current release (version 4.1) adds a number of new features to
those available in the previous version (3.0). Visible and functional
changes are:

\begin{itemize}

\item Distance weighting of the $k$ nearest neighbours. This classic
exemplar weighting scheme \cite{Dudani76} allows closer nearest
neighbours in the $k$ to have a more prominent vote in
classification. TiMBL incorporates linear, inversed, and exponential
distance weighting.

\item Incremental edited memory-based learning with {\sc ib2}
\cite{Aha+91}. This incremental version of {\sc ib1} adds instances to
memory only when those instances are misclassified by the current set
of instances in memory.

\item Exemplar weighting. TiMBL can read additional numeric exemplar
weights (generated externally) from a data file, and use these
weights in the class voting stages of $k$-NN classification.

\item Cross-validation testing. This option allows TiMBL to run
systematic tests on different values of parameters, without completely
re-initializing the classifier in every fold of the validation experiment.

\end{itemize}

A more elaborate description of the changes from version 1.0 up to 4.1
can be found in Chapter~\ref{changes}. Although these new features
have been tested for some time in our research group, the software may
still contain bugs and inconsistencies in some places. We would
appreciate it if you would send bug reports, ideas about enhancements
of the software and the manual, and any other comments you might have,
to {\tt Timbl@kub.nl}.

This reference guide is structured as follows. In
Chapter~\ref{license} you can find the terms of the license according
to which you are allowed to use TiMBL. The subsequent chapter gives
some instructions on how to install the TiMBL package on your
computer. Chapter~\ref{changes} lists the changes that have taken
place up to the current version. Next, Chapter~\ref{tutorial} offers a
quick-start tutorial for readers who want to get to work with TiMBL
right away. The tutorial describes, step-by-step, a case study with a
sample data set (included with the software) representing the
linguistic domain of predicting the diminutive inflection of Dutch
nouns.  Readers who are interested in the theoretical and technical
details of Memory-Based Learning and of this implementation can refer
to Chapter~\ref{algorithms}. Chapters~\ref{fileformats}
and~\ref{commandline} provide, respectively, a full reference to the
file formats and command line options of TiMBL.

\chapter{License terms}
\label{license}
\pagenumbering{arabic} 

Downloading and using the TiMBL software implies that you accept the
following license terms:

Tilburg University and University of Antwerp (henceforth
``Licensers'') grant you, the registered user (henceforth ``User'')
the non-exclusive license to download a single copy of the TiMBL
program code and related documentation (henceforth jointly referred to
as ``Software'') and to use the copy of the code and documentation
solely in accordance with the following terms and conditions:

\begin{itemize}

\item The license is only valid when you register as a user. If you
have obtained a copy without registration, you must immediately
register by sending an e-mail to {\tt Timbl@kub.nl}.

\item User may only use the Software for educational or non-commercial
research purposes.

\item Users may make and use copies of the Software internally for
their own use.

\item Without executing an applicable commercial license with
Licensers, no part of the code may be sold, offered for sale, or made
accessible on a computer network external to your own or your
organization's in any format; nor may commercial services utilizing
the code be sold or offered for sale. No other licenses are granted or
implied.

\item Licensers have no obligation to support the Software it is
providing under this license.  To the extent permitted under the
applicable law, Licensers are licensing the Software "AS IS", with no
express or implied warranties of any kind, including, but not limited
to, any implied warranties of merchantability or fitness for any
particular purpose or warranties against infringement of any
proprietary rights of a third party and will not be liable to you for
any consequential, incidental, or special damages or for any claim by
any third party.

\item Under this license, the copyright for the Software remains the
joint property of the ILK Research Group at Tilburg University, and
the CNTS Research Group at the University of Antwerp.  Except as
specifically authorized by the above licensing agreement, User may not
use, copy or transfer this code, in any form, in whole or in part.

\item Licensers may at any time assign or transfer all or part of their
interests in any rights to the Software, and to this license, to an
affiliated or un-affiliated company or person.

\item Licensers shall have the right to terminate this license at any
time by written notice. User shall be liable for any infringement or
damages resulting from User's failure to abide by the terms of this
License.

\item In publication of research that makes use of the Software, a
citation should be given of: {\em ``Walter Daelemans, Jakub Zavrel, Ko
van der Sloot, and Antal van den Bosch (2001). TiMBL: Tilburg Memory
Based Learner, version 4.1, Reference Guide. ILK Technical Report
01-04, Available from {\tt
http://ilk.kub.nl/downloads/pub/papers/ilk0104.ps.gz}}

\item For information about commercial licenses for the Software,
contact {\tt Timbl@kub.nl}, or send your request in writing to:

Prof.dr.~Walter Daelemans\\
CNTS - Language Technology Group\\
GER / University of Antwerp\\
Universiteitsplein 1\\
B-2610 Wilrijk (Antwerp)\\
Belgium

\end{itemize}

\pagestyle{headings}

\chapter{Installation}
\vspace{-1cm}
You can get the TiMBL package as a gzipped tar archive from:

{\tt http://ilk.kub.nl/software.html}

Following the links from that page, you will be required to fill in
registration information and to accept the license agreement. You can
proceed to download the file {\tt Timbl.4.1.tar.gz}. This file contains
the complete source code (C++) for the TiMBL program, a few sample
data sets, the license and the documentation. The installation should
be relatively straightforward on most UNIX systems.

To install the package on your computer, unzip the downloaded file:

{\tt > gunzip Timbl.4.1.tar.gz}

and unpack the tar archive:

{\tt > tar -xvf Timbl.4.1.tar}

This will make a directory {\tt Timbl.4.1} under your 
current directory. Change directory to this:

{\tt > cd Timbl.4.1}

and compile the executable by typing {\tt make}\footnote{We have
tested this with {\tt gcc} versions 2.95.2 and 3.0.}, assuming that
{\tt make} is actually {\tt gnumake} on your system.  Solaris users
should use {\tt gnumake} explicitly, since their {\tt make} defaults
to SunOS {\tt make}, which does not operate correctly.

If the process was completed successfully, you should now have
executable files named {\tt Timbl, Client, tse, classify}, and a
static library {\tt libTimbl.a}.

The e-mail address for problems with the installation, bug reports,
comments and questions is {\tt Timbl@kub.nl}.

\chapter{Changes}
\label{changes}

This chapter gives a brief overview of the changes from all previously
released versions (1.0 up to 4.1) for users already familiar with the
program.

\section{From version 3.0 to 4.1}

Version 4.1 is an intermediate upgrade of version 4.0. Changes
from 4.0 to 4.1 are found at the bottom of this list and are marked as such.

\begin{itemize}

\item Distance weighting of the $k$ nearest neighbours. This classical
exemplar weighting scheme \cite{Dudani76} allows closer nearest
neighbours in the $k$ to have a more prominent vote in
classification. TiMBL incorporates linear, inversed, and exponential
distance weighting.

\item Incremental edited memory-based learning with {\sc ib2}
\cite{Aha+91}. This incremental version of {\sc ib1} adds instances to
only when those instances are misclassified by the then-current set of
instances in memory.

\item Exemplar weighting. TiMBL can read additional numeric exemplar
weights (generated externally) when reading a data file, and use these
weights in the class voting stages of $k$-NN classification.

\item Cross-validation testing. Analogous to the leave-one-out testing
option, with cross-validation testing it is possible to let TiMBL run
systematic tests on different values of parameters, without completely
re-initializing the classifier in every fold of the validation experiment.

\item The command line interface has had four additions reflecting
the above changes, plus one extra verbosity option:

\begin{itemize} 
	\item the {\tt -d metriccode} option sets the distance 
              weighting metric. Three metrics are available: 
              inverse distance (code ID), inverse linear (IL),
              and exponential decay (ED, which takes an extra
              argument $a$, without whitespace, determining the
              factor of the exponential function). By default,
              no distance weighting is used (code Z). See
              Chapter~\ref{algorithms} for descriptions.
	\item the {\tt -a 3} switch invokes the {\sc ib2} algorithm.
              This algorithm expects to have the {\tt -b} switch set:
	\item the {\tt -b n} option sets the number ($n$) of lines
              counting from the top of the training set file, which form
              the bootstrap set of memorized instances to which {\sc ib2} 
              will start adding instances incrementally.
	\item the {\tt +v/-v} option has {\tt cm} as a new optional
              argument; it returns the confusion matrix, obtained
              after testing, between predicted and actual classes in 
              the test data.
\end{itemize}

\item The ``programmer's reference'' or API section has been separated
from this manual. A new API, describing the underlying structure of
TiMBL (which has changed considerably from version 3.0 to 4.1), will
be available upon request (to {\tt Timbl@kub.nl}) from version 4.1
onwards.

\item Two bugs relating to a type of sparse data have been
resolved. The first involved leave-one-out experiments on data sets
with features that have values that occur only once in the training
data. The second bug occurred with the use of the {\tt -F Binary}
option with the same type of data.

\item {\bf [Change from 4.0 to 4.1]} A bug in discretizing numeric
features during feature weighting has been resolved, causing TiMBL's
behavior on numeric features to be different from previous versions in
some cases.

\item {\bf [Change from 4.0 to 4.1]} Exemplar weights are now stored
in the TiMBL-tree.

\item {\bf [Change from 4.0 to 4.1]} The core representation of
TiMBL-trees has been modified, causing no changes at the surface
except that the {\sc tribl} variant uses less memory.

\end{itemize}

\section{From version 2.0 to 3.0}

\begin{itemize}

\item Server functionality. Apart from the standard processing of test
items from a file, alternatively you can now specify a portnumber with
{\tt -S portnumber} to open a socket and send commands for
classification of test patterns or change of parameters to it. A
sample client program is included in the distribution. This allows
fast response times when small amounts of test material are presented
at various intervals. It also opens the possibility of having large
numbers of ``classification agents'' cooperate in real time, or of
classication of the same data with different parameters. The syntax of
our simple Client/Server protocol is described in
Chapter~\ref{serverformat}.

\item Leave-one-out testing. To get an estimate of the classification
error, without setting aside part of one's data as a test set, one
can now test by ``leave-one-out'' ({\tt -t leave\_one\_out}), in effect
testing on every case once, while training on the rest of the cases,
without completely re-initializing the classifier for every test case.

\item Support for sparse binary features. For tasks with large numbers
of sparse binary features, TiMBL now allows for an input format which
lists only the ``active'' features, avoiding the listing of the many
(zero-valued) features for each case. This format is described in
Section~\ref{binaryformat}.

\item Additional feature weighting metrics. We have added chi-squared
and shared variance measures as weighting schemes. These weighting
metrics are sometimes more robust to large numbers of feature values
and other forms of data sparseness.

\item Different metrics (Overlap, {\sc mvdm} or Numeric) can be
applied to different features.

\item The command line interface has slightly been cleaned up, and
re-organized:

\begin{itemize}

\item The {\tt -m metricnumber} switch to choose metrics has been
replaced by the use of a specification string following {\tt
-m}. E.g.~you can specify to use {\sc mvdm} as the default metric, but
use Overlap on features 5-7,9, Numeric on feature 1, and ignore
feature 10 ({\tt -m M:O5-7,9:N1:I10}).

\item All of the output needed for analysing the matching of nearest
neighbors has been moved to the verbosity setting.

\item Verbosity levels and some other options can be switched on {\tt
+v} and off {\tt -v}, even between different classification actions.

\item Because of the large amount of verbosity levels, the {\tt +v}
option takes mnemonic abbreviations as arguments instead of numeric
verbosity levels. Although the old (numeric) format is still
supported, it's use is not encouraged as it will disappear in future
versions.

\end{itemize}

\item Because of significant optimizations in the nearest neighbor
search, the default is no longer to use inverted indexes. These can
however still be turned on by using the {\tt +-} switch on the command
line.

\item You can now choose the output filename or have it generated by
TiMBL on the basis of the test filename and the parameters.

\item You can use TiMBL in a pipeline of commands by specifying '-' as
either input, output or both.

\item Several problems with the display of nearest neighbors in the
output have been fixed.

\item The API has been adapted a bit to allow more practical use of
it.

\end{itemize}

\section{From version 1.0 to 2.0}

\begin{itemize}

\item We have added a new algorithm: {\sc tribl}, a hybrid between
the fast {\sc igtree} algorithm and real nearest neighbor search (for
more details, see~\ref{tribl}, or~\namecite{Daelemans+97d}). This
algorithm is invoked with the {\tt -a 2} switch and requires the
specification of a so-called {\sc tribl}-offset, the feature where
{\sc igtree} stops and case bases are stored under the leaves of the
constructed tree.

\item Support for numeric features. Although the package has retained
its focus on discrete features, it can now also process numeric
features, scale them, and compute feature weights on them. You
specify which features are numeric with the {\tt -N} option on the
command line.

\item The organization of the code is much more object-oriented than
in version 1.0. The main benefit of this is that:

\item A Memory-Based Learning API is made available. You can define
Memory-Based classification objects in your own C++ programs and
access all of the functionality of TiMBL by linking to the TiMBL
library.

\item It has become easier to examine the way decisions are made from
nearest neighbors, because several verbosity-levels allow you to dump
similarity values ({\tt -D}), distributions ({\tt -v 16}), and nearest
neighbor sets ({\tt -v 32}) to the output file. The {\tt -d} option
for writing the distributions no longer exists.

\item Better support for the manipulation of {\sc mvdm}
matrices. Using the {\tt -U} and {\tt -u} options it is now possible
to respectively save and read back value difference matrices (see
Section~\ref{mvdmformat}).

\item Both ``pre-stored'' and ``regular'' {\sc mvdm} experiments now
generate filenames with ``{\tt mvd}'' in the suffix. This used to be
``{\tt pvd}'' and ``{\tt mvd}'' respectively.

\item a number of minor bugs have been fixed.

\end{itemize}

\chapter{Quick-start Tutorial}
\label{tutorial}

This quick-start tutorial is meant to get you started with TiMBL
right away. We discuss how to format the data of a task to serve as
training examples, which choices can be made during the construction
of the classifier, how various choices can be evaluated in terms of
their generalization accuracy, and various other practical issues. The
reader who is interested in more background information on TiMBL
implementation issues and a formal description of Memory-Based
Learning, is advised to read Chapter~\ref{algorithms}.

Memory-Based Learning ({\sc mbl}) is based on the idea that
intelligent behavior can be obtained by analogical reasoning, rather
than by the application of abstract {\em mental rules} as in rule
induction and rule-based processing. In particular, {\sc mbl} is
founded in the hypothesis that the extrapolation of behavior from
stored representations of earlier experience to new situations, based
on the similarity of the old and the new situation, is of key
importance.

{\sc mbl} algorithms take a set of examples (fixed-length patterns of
feature-values and their associated class) as input, and produce a
{\em classifier} which can classify new, previously unseen, input
patterns. Although TiMBL was designed with linguistic classification
tasks in mind, it can in principle be applied to any kind of
classification task with symbolic or numeric features and discrete
(non-continuous) classes for which training data is available. As an
example task for this tutorial we go through the application of TiMBL
to the prediction of Dutch diminutive suffixes. The necessary data
sets are included in the TiMBL distribution, so you can replicate the
examples given below on your own system.

\section{Data}

The operation of TiMBL will be illustrated below by means of a real
natural language processing task: prediction of the diminutive suffix
form in Dutch~\cite{Daelemans+97e}. In Dutch, a noun can receive a
diminutive suffix to indicate {\em small size} literally or
metaphorically attributed to the referent of the noun; e.g. {\em
mannetje} means {\em little man}. Diminutives are formed by a
productive morphological rule which attaches a form of the Germanic
suffix {\em -tje} to the singular base form of a noun. The suffix
shows variation in its form (Table \ref{variation}). The task we
consider here is to predict which suffix form is chosen for previously
unseen nouns on the basis of their form.

\begin{table}[h]
\begin{center}
\begin{tabular}{l|l|l}
Noun & Form & Suffix \\
\hline
huis (house) & huisje & {\em -je} \\
man (man) & mannetje & {\em -etje\/} \\
raam (window) & raampje & {\em -pje\/} \\
woning (house) & woninkje & {\em -kje\/} \\
baan (job) & baantje & {\em -tje\/} \\
\end{tabular}
\caption{Allomorphic variation in Dutch diminutives.}\label{variation}
\end{center}
\end{table}

For these experiments, we collect a representation of nouns in terms
of their syllable structure as training material\footnote{These words
were collected form the {\sc celex} lexical
database~\cite{Baayen+93}}. For each of the last three syllables of
the noun, four different features are collected: whether the syllable
is stressed or not (values - or +), the string of consonants before
the vocalic part of the syllable (i.e. its onset), its vocalic part
(nucleus), and its post-vocalic part (coda). Whenever a feature value
is not present (e.g. a syllable does not have an onset, or the noun
has less than three syllables), the value `=' is used. The class to be
predicted is either E (-etje), T (-tje), J (-je), K (-kje), or P
(-pje).

Some examples are given below (the word itself is only provided for
convenience and is not used). The values of the syllabic content
features are given in phonetic notation.

\begin{table}[h]
\begin{center}
\begin{tabular}{|cccccccccccc|l|l|l|}
\hline
- & b & i & = & - & z & @ & = & + & m & A & nt & J & biezenmand \\
= & = & = & = & = & = & = & = & + & b & I & x & E & big\\
= & = & = & = & + & b & K & = & - & b & a & n & T & bijbaan\\
= & = & = & = & + & b & K & = & - & b & @ & l & T & bijbel\\
\hline
\end{tabular}
\end{center}
\end{table}

Our goal is to use TiMBL in order to train a classifier that can
predict the class of new, previously unseen words as correctly as
possible, given a set of training examples that are described by the
features given above. Because the basis of classification in TiMBL is
the storage of all training examples in memory, a test of the
classifier's accuracy must be done on a separate test set. We will
call these datasets {\tt dimin.train} and {\tt dimin.test},
respectively. The training set {\tt dimin.train} contains 2999 words
and the test set contains 950 words, none of which are present in the
training set. Although a single train/test partition suffices here for
the purposes of explanation, it does not factor out the bias of
choosing this particular split. Unless the test set is sufficiently
large, a more reliable generalization accuracy measurement is used in
real experiments, e.g.~10-fold cross-validation~\cite{Weiss+91}. This
means that 10 separate experiments are performed, and in each ``fold''
90\% of the data is used for training and 10\% for testing, in such a
way that each instance is used as a test item exactly once. Another
reliable way of testing the real error of a classifier is
leave-one-out~\cite{Weiss+91}. In this approach, every data item in
turn is selected once as a test item, and the classifier is trained on
all remaining items. Accuracy of the classifier is then the number of
data items correctly predicted. With the option {\tt -t
leave\_one\_out}, this testing methodology is used by TiMBL. We
will use this option in the tutorial on the file {\tt dimin.data}, the
union of {\tt dimin.train} and {\tt dimin.test}. 

\section{Using TiMBL}

Different formats are allowed for training and test data files. TiMBL
is able to guess the type of format in most cases. We will use
comma-separated values here, with the class as the last value. This
format is called C4.5 format in TiMBL because it is the same as that
used in Quinlan's well-known C4.5 program for induction of decision
trees~\cite{Quinlan93}. See Section~\ref{fileformats} for more
information about this and other file formats.

An experiment is started by executing TiMBL with the two files ({\tt
dimin.train} and {\tt dimin.test}) as arguments:

{\small
\begin{verbatim}
Timbl -f dimin.train -t dimin.test
\end{verbatim}
}

Upon completion, a new file has been created with name\\ {\tt
dimin.test.mbl.O.gr.k1.out}, which is in essence identical to the
input test file, except that an extra comma-separated column is added
with the class predicted by TiMBL. The name of the file provides
information about the {\sc mbl} algorithms and metrics used in the
experiment (the default values in this case). We will describe these
shortly.

Apart from the result file, information about the operation of the
algorithm is also sent to the standard output. It is therefore 
advisable to redirect the output to a file in order to make a log of
the results.

{\small
\begin{verbatim}
Timbl -f dimin.train -t dimin.test > dimin-exp1
\end{verbatim}
}

The defaults used in this case work reasonably well for most problems.  We
will now provide a point by point explanation of what goes on in the
output.

\vspace{1cm}

\rule{\textwidth}{0.5mm}

{\small
\begin{verbatim}
TiMBL 4.1 (c) ILK 1998, 1999, 2000, 2001.
Tilburg Memory-Based Learner
Induction of Linguistic Knowledge Research Group
Tilburg University / University of Antwerp
Tue Feb 29 15:23:16 2000
 
Examine datafile gave the following results:
Number of Features: 12
InputFormat       : C4.5               
\end{verbatim}
}

\rule{\textwidth}{0.5mm}

\vspace{1cm}

TiMBL has detected 12 features and the C4.5 input format
(comma-separated features, class at the end).

\rule{\textwidth}{0.5mm}

{\small
\begin{verbatim}
Phase 1: Reading Datafile: dimin.train
Start:          0 @ Tue Feb 29 15:23:16 2000
Finished:    2999 @ Tue Feb 29 15:23:16 2000
Calculating Entropy         Tue Feb 29 15:23:16 2000
Lines of data     : 2999
DB Entropy        : 1.6178929
Number of Classes : 5
 
Feats   Vals    X-square        Variance        InfoGain        GainRatio
    1      3    128.41828       0.021410184     0.030971064     0.024891536
    2     50    364.75812       0.030406645     0.060860038     0.027552191
    3     19    212.29804       0.017697402     0.039562857     0.018676787
    4     37    449.83823       0.037499019     0.052541227     0.052620750
    5      3    288.87218       0.048161417     0.074523225     0.047699231
    6     61    415.64113       0.034648310     0.10604433      0.024471911
    7     20    501.33465       0.041791818     0.12348668      0.034953203
    8     69    367.66021       0.030648567     0.097198760     0.043983864
    9      2    169.36962       0.056475363     0.045752381     0.046816705
   10     64    914.61906       0.076243669     0.21388759      0.042844587
   11     18    2807.0418       0.23399815      0.66970458      0.18507018
   12     43    7160.3682       0.59689631      1.2780762       0.32537181
 
Feature Permutation based on GainRatio/Values :
< 9, 5, 11, 1, 12, 7, 4, 3, 10, 8, 2, 6 >                  
\end{verbatim}
}

\rule{\textwidth}{0.5mm}

\vspace{1cm}

Phase 1 is the training data analysis phase. Time stamps for start and
end of analysis are provided. Some preliminary analysis of the
training data is done: number of training items, number of classes,
entropy of the training data. For each feature, the number of values,
and four variants of an information-theoretic measure of feature
relevance are given. These are used both for memory organization
during training and for feature relevance weighting during testing
(see Chapter~\ref{algorithms}). Finally, an ordering (permutation) of
the features is given. This ordering is used for building the
tree-index to the case-base.

\vspace{1cm}

\rule{\textwidth}{0.5mm}


{\small
\begin{verbatim}
Phase 2: Learning from Datafile: dimin.train
Start:          0 @ Tue Feb 29 15:23:16 2000
Finished:    2999 @ Tue Feb 29 15:23:16 2000
 
Size of InstanceBase = 19231 Nodes, (384620 bytes), 49.77 % compression           
\end{verbatim}
}

\rule{\textwidth}{0.5mm}

\vspace{1cm}

Phase 2 is the learning phase; all training items are stored in an
efficient way in memory for use during testing. Again timing
information (real time) is provided, as well as information about the
size of the data structure representing the stored examples and the
amount of compression achieved. 

\vspace{1cm}

\rule{\textwidth}{0.5mm}

{\small
\begin{verbatim}
Starting to test, Testfile: dimin.test
Writing output in:          dimin.test.mbl.O.gr.k1.out
Algorithm     : IB1
Global metric : Overlap 
Deviant Feature Metrics:
Weighting     : GainRatio
 
Tested:      1 @ Tue Feb 29 15:23:16 2000
Tested:      2 @ Tue Feb 29 15:23:16 2000
Tested:      3 @ Tue Feb 29 15:23:16 2000
Tested:      4 @ Tue Feb 29 15:23:16 2000
Tested:      5 @ Tue Feb 29 15:23:16 2000
Tested:      6 @ Tue Feb 29 15:23:16 2000
Tested:      7 @ Tue Feb 29 15:23:16 2000
Tested:      8 @ Tue Feb 29 15:23:16 2000
Tested:      9 @ Tue Feb 29 15:23:16 2000
Tested:     10 @ Tue Feb 29 15:23:16 2000
Tested:    100 @ Tue Feb 29 15:23:16 2000
Ready:     950 @ Tue Feb 29 15:23:16 2000
Seconds taken: 1 (950.00 p/s)
918/950 (0.966316), of which 39 exact matches
 
There were 5 ties of which 3 (60.00%) were correctly resolved          
\end{verbatim}
}

\rule{\textwidth}{0.5mm}

\vspace{1cm}

In Phase 3, the trained classifier is applied to the test set. Because
we have not specified which algorithm to use, the default settings are
used ({\sc ib1} with information theoretic feature weighting). This
algorithm computes the similarity between a test item and each
training item in terms of {\em weighted overlap}: the total difference
between two patterns is the sum of the relevance weights of those
features which are not equal. The class for the test item is decided
on the basis of the least distant item(s) in memory. To compute
relevance, Gain Ratio is used (an information-theoretic measure, see
Section~\ref{infogain}). Time stamps indicate the progress of the
testing phase. Finally, accuracy on the test set is logged, and the
number of exact matches\footnote{An exact match in this experiment can
occur when two different nouns have the same feature-value
representation.} and ties (two or more classes are equally frequent in
the nearest neighbor set). In this experiment, the diminutive suffix
form of 96.6\% of the new words was correctly predicted. Train and
test set overlap in 39 items, and the algorithm had to break 5 ties,
of which 3 correctly.

The meaning of the output file names can be explained now:\\ {\tt
dimin.test.mbl.O.gr.k1.out} means output file ({\tt .out}) for {\tt
dimin.test} with algorithm {\sc mbl} (={\sc ib1}), similarity computed
as {\em weighted overlap} ({\tt .O}), relevance weights computed with
{\em gain ratio} ({\tt .gr}), and number of most similar memory
patterns on which the output class was based equal to 1 ({\tt .k1}).

\section{Algorithms and Metrics}

A precise discussion of the different algorithms and metrics
implemented in TiMBL is given in Chapter~\ref{algorithms}. We will
discuss the effect of the most important ones on our data set.

A first choice in algorithms is between using {\sc ib1} and {\sc
igtree}. In the trade-off between generalization accuracy and
efficiency, {\sc ib1} usually, but not always, leads to more accuracy
at the cost of more memory and slower computation, whereas {\sc
igtree} is a fast heuristic approximation of {\sc ib1}, but sometimes
less accurate. The {\sc igtree} algorithm is used when {\tt -a 1} is
given on the command line, whereas the {\sc ib1} algorithm used above
(the default) would have been specified explicitly by {\tt -a 0}. 

{\small
\begin{verbatim}
Timbl -a 1 -f dimin.train -t dimin.test
\end{verbatim}} 

We see that {\sc igtree} performs only slightly worse than {\sc ib1}
for this train-test partitioning of the data (it uses less memory and
is faster, however). 

When using the {\sc ib1} algorithm, there is a choice of metrics for
influencing the definition of similarity. With {\em weighted overlap},
each feature is assigned a weight, determining its relevance in
solving the task. With the {\em modified value difference metric}
({\sc mvdm}), each pair of values of a particular feature is assigned
a value difference. The intuition here is that in our diminutive
problem, for example, the codas $n$ and $m$ should be regarded as
being more similar than $n$ and $p$. These pair-wise differences are
computed for each pair of values in each feature (see
Section~\ref{mvdm}). Selection between weighted overlap and {\sc mvdm}
is done by means of the {\tt -mM} parameter. The following selects {\sc
mvdm}, whereas {\tt -mO} ({\em weighted overlap}) is the default.

{\small
\begin{verbatim}
Timbl -mM -f dimin.train -t dimin.test
\end{verbatim}
}

Especially when using {\sc mvdm}, but also in other cases, it may be
useful to extrapolate not just from the most similar example in
memory, which is the default, but from several. This can be achieved
by using the $-k$ parameter followed by the wanted number of nearest
neighbors. E.g., the following applies {\sc ib1} with the {\sc mvdm}
metric, with extrapolation from the 5 nearest neighbors.

{\small
\begin{verbatim}
Timbl -mO -k 5 -f dimin.train -t dimin.test
\end{verbatim}
}

Whenever more than one nearest neighbor is taken into account for
extrapolation, it may be useful to weigh the influence of the
neighbors on the final decision as a function of their distance from
the test item. Several possible implementations of this distance
function are provided. E.g., the following provides inverse distance: 

{\small
\begin{verbatim}
Timbl -mO -k 5 -f dimin.train -t dimin.test -d ID
\end{verbatim}
}

Within the {\sc ib1} {\em weighted overlap} option, the default
feature weighting method is Gain Ratio. Other feature relevance
weighting methods are available as well.  By setting the parameter
{\tt -w} to 0, an {\em overlap} definition of similarity is created
where each feature is considered equally relevant. Similarity reduces
in that case to the number of equal values in the same position in the
two patterns being compared. As an alternative weighting, users can
provide their own weights by using the {\tt -w} parameter with a
filename in which the feature weights are stored (see
Section~\ref{weightformat} for a description of the format of the
weights file).

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|r|r|}\hline
             & overlap & gain ratio & information gain & X2 & Inverse Linear\\
\hline
{\sc ib1}, $-k1$ & 87.2 & 96.0 & 96.0 & 95.9 & \\
{\sc ib1}, $-k3$ & 73.6 & 95.9 & 96.5 & 96.5 & 96.6 \\
{\sc mvdm}, $-k1$ & 95.1  & 95.8 & 95.8 & 95.8 & \\
{\sc mvdm}, $-k5$ & 97.1 & 97.4 & {\bf 97.5} & {\bf 97.5} & 96.5\\
\hline
\end{tabular}
\caption{Some results for diminutive prediction.}
\label{diminresults}
\end{center}
\end{table}

Table \ref{diminresults} shows the effect of algorithm, metric,
distance weighting of nearest neighbors, and weighting method choice
on generalization accuracy using leave-one-out as experimental method.

{\small
\begin{verbatim}
Timbl -t leave_one_out -f dimin.data
\end{verbatim}
}

When comparing {\sc mvdm} and {\sc ib1}, we see that the overall best
results are achieved with {\sc mvdm}, but only with a higher value for
$k$, the number of memory items (actually distances) on which the
extrapolation is based. Increasing the value of $k$ for (weighted)
Overlap metrics decreased performance. Within the feature weighting
approaches, overlap (i.e. no weighting) performs markedly worse than
the default {\em information gain}, {\em gain ratio} or {\em
chi-square} weighting methods. 

The default settings provided in TiMBL were selected on the basis of
our experience with a large set of (mostly linguistic)
datasets. However, as can be seen from this dataset, they are not
guaranteed to be the best choice (which is 1.5\% better for the best
of the options explored in Table \ref{diminresults}). It is always
useful to try out a large set of reasonable combinations of options by
cross-validation on the training data to achieve best results with
{\sc mbl}. The option {\tt -t @f} where {\tt f} is the name of a file,
allows you to predefine various combinations of options to be tested
and test them without having the training stages repeated each
time. See Chapter \ref{commandline}.

\section{More Options}

Several input and output options exist to make life easier while
experimenting. See Chapter~\ref{commandline} for a detailed
description of these options. One especially useful option for testing
linguistic hypotheses is the ignore option, which allows you to skip
certain features when computing similarity. E.g. if we want to test
the hypothesis that only the rime (nucleus and coda) and the stress of
the last syllable are actually relevant in determining the form of the
diminutive suffix, we can execute the following with the previously
best parameter settings to disregard all but the fourth-last and the
last two features. As a result we get an accuracy of
97.0\%\footnote{It should be kept in mind that the amount of overlap
in training and test set has significantly increased, so that
generalization is based on retrieval more than on similarity
computation.}. 

{\small
\begin{verbatim}
Timbl -mM:I1-8,10 -f dimin.data -t leave_one_out -k5 -w3
\end{verbatim}
}


The {\sc +/-v} (verbosity) option allows you to control the amount of
information that is generated in the output, ranging from nothing much
({\sc +v s}) to a lot ({\sc +v o+p+e+cm+di+db+n}). Specific verbosity
settings exist for dumping option settings ({\sc +v o}), feature
relevance weights (default), value-class conditional probabilities
({\sc +v p}), exact matches ({\sc +v e}), distributions ({\sc +v db}),
a confusion matrix ({\sc +v cm}) and the nearest neighbors on which
decision are based ({\sc +v n}) or the distances to the nearest
neighbor ({\sc +v di}). E.g. the following command results in an
output file with distributions.

{\small
\begin{verbatim}
Timbl +v db -f dimin.train -t dimin.test
\end{verbatim}
}

The resulting output file contains lines like the following.

{\small
\begin{verbatim}
=,=,=,=,+,=,K,=,-,s,A,k,J,J { J 1.00000 }
-,r,i,=,+,j,o,=,-,d,@,=,T,T { T 4.00000, E 1.00000, J 1.00000 }
+,vr,a,=,-,G,@,=,-,l,K,st,J,J { J 1.00000 }
=,=,=,=,=,=,=,=,+,b,o,m,P,P { P 3.00000 }
-,t,@,=,-,G,@,=,-,z,I,xt,J,J { J 1.00000 }
=,=,=,=,-,p,A,=,+,tr,o,n,T,T { T 1.00000 }
+,k,I,n,-,d,@,r,-,b,E,t,J,J { J 1.00000 }
+,s,L,=,-,k,@,r,-,b,e,st,J,J { J 1.00000 }
=,=,=,=,+,sx,I,=,-,m,@,l,T,T { T 12.0000 }
=,=,=,=,=,=,=,=,+,kl,M,n,T,T { T 43.0000, E 20.0000 }
\end{verbatim}
}

This information can e.g. be used to assign a certainty to a decision
of the classifier, or to make available a second-best back-off option.

{\small
\begin{verbatim}
Timbl +v di -f dimin.train -t dimin.test
\end{verbatim}
}

{\small
\begin{verbatim}
+,l,a,=,-,d,@,=,-,k,A,st,J,J        0.070701
-,s,i,=,-,f,E,r,-,st,O,k,J,J        0.000000
=,=,=,=,=,=,=,=,+,sp,a,n,T,T        0.042845
=,=,=,=,=,=,=,=,+,st,o,t,J,J        0.042845
=,=,=,=,+,sp,a,r,-,b,u,k,J,J        0.024472
+,h,I,N,-,k,@,l,-,bl,O,k,J,J        0.147489
-,m,e,=,-,d,A,l,+,j,O,n,E,E         0.182421
-,sn,u,=,-,p,@,=,+,r,K,=,T,T        0.046229
=,=,=,=,=,=,=,=,+,sp,A,N,E,E        0.042845
+,k,a,=,-,k,@,=,-,n,E,st,J,J        0.114685        
\end{verbatim}
}

This can be used to study how very similar instances (low distance) and
less similar patterns (higher distance) are used in the process of
generalization.

The listing of nearest neighbors is useful for the analysis of the
behavior of a classifier. It can be used to interpret why particular
decisions or errors occur.

{\small
\begin{verbatim}
Timbl +v n -f dimin.train -t dimin.test
\end{verbatim}
}

{\small
\begin{verbatim}
-,t,@,=,-,l,|,=,-,G,@,n,T,T
# k=1, 1 Neighbor(s) at distance: 0.0997233
#       -,x,@,=,+,h,|,=,-,G,@,n, -*-
-,=,I,n,-,str,y,=,+,m,E,nt,J,J
# k=1, 1 Neighbor(s) at distance: 0.123322
#       -,m,o,=,-,n,y,=,+,m,E,nt, -*-
=,=,=,=,=,=,=,=,+,br,L,t,J,J
# k=1, 4 Neighbor(s) at distance: 0.0428446
#       =,=,=,=,=,=,=,=,+,r,L,t, -*-
#       =,=,=,=,=,=,=,=,+,kr,L,t, -*-
#       =,=,=,=,=,=,=,=,+,sx,L,t, -*-
#       =,=,=,=,=,=,=,=,+,fl,L,t, -*-
=,=,=,=,+,zw,A,=,-,m,@,r,T,T
# k=1, 5 Neighbor(s) at distance: 0.0594251
#       =,=,=,=,+,fl,e,=,-,m,@,r, -*-
#       =,=,=,=,+,=,E,=,-,m,@,r, -*-
#       =,=,=,=,+,l,E,=,-,m,@,r, -*-
#       =,=,=,=,+,k,a,=,-,m,@,r, -*-
#       =,=,=,=,+,h,O,=,-,m,@,r, -*-                    
\end{verbatim}
}

{\small
\begin{verbatim}
Timbl +v cm -f dimin.train -t dimin.test
\end{verbatim}
}

{\small
\begin{verbatim}
Confusion Matrix:
             T      E      J      P      K
        -----------------------------------
     T |    453      0      2      0      0
     E |      0     87      4      1      8
     J |      1      5    346      0      0
     P |      0      3      0     24      0
     K |      0      8      0      0      8
                                                  
\end{verbatim}
}

The confusion matrix associates the class predicted by TiMBL
(vertically) with the real class of the test items given
(horizontally). E.g., in this experiment, TiMBL predicted class T
(-tje) for 454 test items, of which 453 were indeed class T, and 1
should have received class J (precision for class T is 99.8\%). Or
viewed differently, of the 455 items in the test set with class T, 453
were correctly predicted by TiMBL, and 2 were incorrectly assigned
class J (recall for class T is 99.6\%). Class E (-etje) has a recall
of 87.0\%, and a precision of 84.5\%.

A confusion matrix allows a more fine-grained analysis of experimental
results and better experimental designs (some parameter settings may
work for some classes but not for others, or some may improve recall,
and others precision, e.g.).

We hope that this tutorial has made it clear that, once you have coded
your data in fixed-length feature-value patterns, it should be
relatively straightforward to get the first results using TiMBL. You
can then experiment with different metrics and algorithms to try and
further improve your results.

\chapter{Memory-based learning algorithms}
\label{algorithms}

TiMBL is a program implementing several memory-based learning
algorithms. All implemented algorithms have in common that they store some
representation of the training set explicitly in memory. During
testing, new cases are classified by extrapolation from the most
similar stored cases. The main differences among the algorithms
incorporated in TiMBL lie in:

\begin{itemize}
\item The definition of {\em similarity},
\item The way the instances are stored in memory, and
\item The way the search through memory is conducted.
\end{itemize}

In this chapter, various choices for these issues are described. We
start in Section~\ref{mbl} with a formal description of the basic
memory-based learning algorithm, i.e.~a nearest neighbor search. We
then introduce different similarity metrics, such as Information Gain
weighting, which allows us to deal with features of differing
importance, and the Modified Value Difference metric, which allows us
to make a graded guess of the match between two different symbolic
values, and describe the standard versus three distance-weighted
versions of the class voting mechanism of the nearest neighbor
classifier. In Section~\ref{indexing}, we give a description of
various algorithmic optimizations for nearest neighbor search.

Sections~\ref{igtree} to~\ref{ib2} describe three variants of the
standard nearest neighbour classifier implemented within TiMBL, that
optimize some intrinsic property of the standard algorithm. First, in
Section~\ref{igtree}, we describe {\sc igtree}, which replaces the
exact nearest neighbor search with a very fast heuristic that exploits
the difference in importance between features. Second, in
Section~\ref{tribl}, we describe the {\sc tribl} algorithm, which is a
hybrid between {\sc igtree} and nearest neighbor search. Third,
Section~\ref{ib2} describes the {\sc ib2} algorithm, which
incrementally and selectively adds instances to memory during
learning.

The chapter is concluded by Section~\ref{furtherreading}, which
provides an overview of further reading into theory and applications
of memory-based learning to natural language processing tasks.

\section{Memory-Based Learning}
\label{mbl}

Memory-based learning is founded on the hypothesis that performance in
cognitive tasks is based on reasoning on the basis of similarity of
new situations to {\em stored representations of earlier experiences},
rather than on the application of {\em mental rules}\/ abstracted from
earlier experiences (as in rule induction and rule-based processing).
The approach has surfaced in different contexts using a variety of
alternative names such as similarity-based, example-based,
exemplar-based, analogical, case-based, in\-stance-based, and lazy
learning~\cite{Stanfill+86,Cost+93,Kolodner93,Aha+91,Aha97a}.
Historically, memory-based learning algorithms are descendants of the
$k$-nearest neighbor (henceforth $k$-{\sc nn}) algorithm
\cite{Cover+67,Devijver+82,Aha+91}.

An {\sc mbl} system, visualized schematically in
Figure~\ref{mbl-method}, contains two components: a {\em learning
component}\/ which is memory-based (from which {\sc mbl} borrows its
name), and a {\em performance component}\/ which is similarity-based.

The learning component of {\sc mbl} is memory-based as it involves
adding training instances to memory (the {\em instance base} or case
base); it is sometimes referred to as `lazy' as memory storage is done
without abstraction or restructuring.  An instance consists of a
fixed-length vector of $n$ feature-value pairs, and an information
field containing the classification of that particular feature-value
vector.  

In the performance component of an {\sc mbl} system, the product of
the learning component is used as a basis for mapping input to output;
this usually takes the form of performing classification.  During
classification, a previously unseen test example is presented to the
system. The similarity between the new instance $X$ and all examples
$Y$ in memory is computed using a {\em distance metric}
$\Delta(X,Y)$. The extrapolation is done by assigning the most
frequent category within the $k$ most similar example(s) as the
category of the new test example.

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=8cm
                \epsffile{mble-method.eps}
                \caption{General architecture of an {\sc mbl} system.
                }
                \label{mbl-method}
        \end{center}
\end{figure}

\subsection{Overlap metric}
\label{overlap}

The most basic metric that works for patterns with symbolic features
is the {\bf Overlap metric}\footnote{This metric is also referred to
as Hamming distance, Manhattan metric, city-block distance, or L1
metric.} given in equations~\ref{distance} and~\ref{overlapeq}; where
$\Delta(X,Y)$ is the distance between patterns $X$ and $Y$,
represented by $n$ features, and $\delta$ is the distance per
feature. The distance between two patterns is simply the sum of the
differences between the features. The $k$-{\sc nn} algorithm with this
metric is called {\sc ib1} \cite{Aha+91}. Usually $k$ is set to 1.

\begin{equation}
\Delta(X,Y) = \sum_{i=1}^{n} \delta(x_{i},y_{i})
\label{distance}
\end{equation}

where:
\begin{equation}
\delta(x_{i}, y_{i}) = \left\{ \begin{array}{ll}
		abs(\frac{x_{i}-y_{i}}{max_{i}-min_{i}}) & \mbox{if numeric, else}\\
		0 & \mbox{if $x_{i} = y_{i}$}\\
		1 & \mbox{if $x_{i} \neq y_{i}$}\\
	\end{array} \right.
\label{overlapeq}
\end{equation}

We have made three additions to the original algorithm \cite{Aha+91} in
our version of {\sc ib1}.  First, in the case of nearest neighbor sets
larger than one instance ($k>1$ or ties), our version of {\sc ib1}
selects the classification that has the highest frequency in the class
distribution of the nearest neighbor set. Second, if a tie cannot be
resolved in this way because of equal frequency of classes among the
nearest neighbors, the classification is selected with the highest
overall occurrence in the training set. Third, in our implementation,
the value of $k$ refers to $k$-nearest distances rather than
$k$-nearest cases.

\subsection{Information Gain feature weighting}
\label{infogain}

The distance metric in equation~\ref{overlapeq} simply counts the
number of (mis)matching feature-values in both patterns. In the
absence of information about feature relevance, this is a reasonable
choice. Otherwise, we can add domain knowledge bias to weight or
select different features (see e.g.~\namecite{Cardie96} for an
application of linguistic bias in a language processing task), or look
at the behavior of features in the set of examples used for
training. We can compute statistics about the relevance of features by
looking at which features are good predictors of the class
labels. Information Theory gives us a useful tool for measuring
feature relevance in this way~\cite{Quinlan86,Quinlan93}.

{\bf Information Gain} (IG) weighting looks at each feature in
isolation, and measures how much information it contributes to our
knowledge of the correct class label. The Information Gain of feature
$i$ is measured by computing the difference in uncertainty
(i.e. entropy) between the situations without and with knowledge of
the value of that feature (equation~\ref{IGgain}).

\begin{equation}
w_{i} = H(C) -  \sum_{v \in V_{i}} P(v) \times H(C|v)
\label{IGgain}
\end{equation}

Where $C$ is the set of class labels, $V_{i}$ is the set of values for
feature $i$, and $H(C) = - \sum_{c \in C} P(c) \log_{2} P(c)$ is the
entropy of the class labels. The probabilities are estimated from
relative frequencies in the training set. 

For numeric features, an intermediate step needs to be taken to apply
the sym bol-based computation of IG. Values are first discretized into
a number (the default is 20) of equally-spaced intervals between the
minimum and maximum values of the feature. These groups are then used
in the IG computation as if they were discrete values. Note that this
discretization is not used in the computation of the distance metric.

It is important to realize that the IG weight is really a
probability-weighted average of the informativity of the different
values of the feature. On the one hand, this pre-empts the
consideration of values with low frequency but high
informativity. Such values ``disappear'' in the average. On the other
hand, this also makes the IG weight very robust to estimation
problems. Each parameter (weight) is estimated on the whole data set.

Information Gain, however, tends to overestimate the relevance of
features with large numbers of values. Imagine a data set of hospital
patients, where one of the available features is a unique ``patient ID
number''. This feature will have very high Information Gain, but it
does not give any generalization to new instances. To normalize
Information Gain for features with different numbers of values,
Quinlan~\cite{Quinlan93} has introduced a normalized version, called
{\bf Gain Ratio}, which is Information Gain divided by $si(i)$ (split info),
the entropy of the feature-values (equation~\ref{splitinfo}).

\begin{equation}
w_{i} = \frac{H(C) -  \sum_{v \in V_{i}} P(v) \times H(C|v)}{si(i)}
\label{IGgainratio}
\end{equation}

\begin{equation}
si(i) = - \sum_{v \in V_{i}} P(v) \log_{2} P(v)
\label{splitinfo}
\end{equation}

The resulting Gain Ratio values can then be used as weights $w_{f}$ in
the weighted distance metric (equation~\ref{distancew})\footnote{In a
generic use IG refers both to Information Gain and to Gain Ratio
throughout this manual. In specifying parameters for the software, the
distinction between both needs to be made, because they often result
in different behavior.}. The $k$-{\sc nn} algorithm with this
metric is called {\sc ib1-ig} \cite{Daelemans+92b}.

\begin{equation}
\Delta(X,Y) = \sum_{i=1}^{n}\ w_{i} \ \delta(x_{i},y_{i})
\label{distancew}
\end{equation} 

The possibility of automatically determining the relevance of features
implies that many different and possibly irrelevant features can be
added to the feature set. This is a very convenient methodology if
domain knowledge does not constrain the choice enough beforehand, or
if we wish to measure the importance of various information sources
experimentally. However, because IG values are computed for each
feature independently, this is not necessarily the best
strategy. Sometimes better results can be obtained by leaving features
out than by letting them in with a low weight. Very redundant features
can also be challenging for {\sc ib1-ig}, because IG will overestimate
their joint relevance. Imagine an informative feature which is
duplicated. This results in an overestimation of IG weight by a factor
two, and can lead to accuracy loss, because the doubled feature will
dominate the similarity metric.

\subsection{Chi-squared feature weighting}
\label{chisquared}

Unfortunately, as~\namecite{White+94} have shown, the Gain Ratio measure
still has an unwanted bias towards features with more values. The
reason for this is that the Gain Ratio statistic is not corrected for
the number of degrees of freedom of the contingency table of classes
and values. \namecite{White+94} proposed a feature selection measure based
on the chi-squared statistic, as values of this statistic can be
compared across conditions with different numbers of degrees of
freedom.

The \chisq statistic is computed from the same contingency table as
the Information Gain measure by the following formula
(Equation~\ref{chisq-eq}).

\begin{equation} 
\chi^{2} = \sum_{i} \sum_{j} \frac{(E_{ij} - O_{ij})^{2}}
				  {E_{ij}} 
\label{chisq-eq}
\end{equation} 

where $O_{ij}$ is the observed number of cases with value $v_{i}$ in
class $c_{j}$, i.e.~$O_{ij} = n_{ij}$, and $E_{ij}$ is the expected
number of cases which should be in cell ($v_{i}$, $c_{j}$) in the
contingency table, if the null hypothesis (of no predictive
association between feature and class) is true
(Equation~\ref{chisq-expect-eq}). Let $n_{.j}$ denote the marginal for
class $j$ (i.e.~the sum over column $j$ of the table), $n_{i.}$ the
marginal for value $i$, and $n_{..}$ the total number of cases
(i.e.~the sum of all the cells of the contingency table).

\begin{equation}
E_{ij} = \frac{n_{.j} n_{i.}}{n_{..}}
\label{chisq-expect-eq}
\end{equation}

The \chisq statistic is well approximated by the chi-square
distribution with $\nu = (m-1)(n-1)$ degrees of freedom, where $m$ is
the number of values and $n$ is the number of classes. We can then
either use the \chisq values as feature weights in
Equation~\ref{distancew}, or we can explicitly correct for the of
degrees of freedom by using the {\bf Shared Variance} measure
(Equation~\ref{shared-variance-eq}). 

\begin{equation}
SV_{i} = \frac{ \chi^2_{i}}{N \times min(|C|,|V|)-1}
\label{shared-variance-eq}
\end{equation}

Where $|C|$ and $|V|$ are the number of classes and the number of values
respectively. We will refer to these variations of {\sc mbl} as {\sc
ib1-\chisq} and {\sc ib1-sv}.

One should keep in mind, that the correspondence to the chi-square
distribution generally becomes poor if the expected frequencies in the
contingency table cells become small. A common recommendation is that
the \chisq test cannot be trusted when more than $20\%$ of the
expected frequencies are less than $5$, or any are less than $1$.

Chi-squared and shared variance weights of {\em numeric}\/ features are
computed via a discretization preprocessing step (also used with
computing IG and GR weights). Values are first discretized into a
number (20 by default) of equally-spaced intervals between the
minimum and maximum values of the feature. These groups are then used
as discrete values in computing chi-squared and shared variance weights.

\subsection{Modified Value Difference metric}
\label{mvdm}

It should be stressed that the choice of representation for instances
in {\sc mbl} remains the key factor determining the strength of the
approach. The features and categories in NLP tasks are usually
represented by symbolic labels. The metrics that have been described
so far, i.e.~Overlap and IG Overlap, are limited to exact match
between feature-values. This means that all values of a feature are
seen as equally dissimilar. However, if we think of an imaginary task
in e.g.~the phonetic domain, we might want to use the information that
'b' and 'p' are more similar than 'b' and 'a'. For this purpose a
metric was defined by \namecite{Stanfill+86} and further refined by
\namecite{Cost+93}. It is called the (Modified) Value Difference
Metric ({\sc mvdm}; equation~\ref{MVDMeq}), and it is a method to
determine the similarity of the values of a feature by looking at
co-occurrence of values with target classes. For the distance between
two values $V_{1},\ V_{2}$ of a feature, we compute the difference of
the conditional distribution of the classes $C_{i}$ for these values.

\begin{equation}
\delta(V_{1}, V_{2}) =e \sum_{i=1}^{n} \left| P(C_{i}|V_{1}) - P(C_{i}|V_{2})
\right|
\label{MVDMeq}
\end{equation}

For computational efficiency, all pairwise $\delta(V_{1}, V_{2})$
values can be pre-comput\-ed before the actual nearest neighbor search
starts. Note that for numeric features, no {\sc mvdm} is computed in
TiMBL, but a scaled difference (see Equation~\ref{overlapeq}) of the
actual numeric feature values.

Although the {\sc mvdm} metric does not explicitly compute feature
relevance, an implicit feature weighting effect is present. If
features are very informative, their conditional class probabilities
will on average be very skewed towards a particular class. This
implies that on average the $\delta(V_{1}, V_{2})$ will be large. For
uninformative features, on the other hand, the conditional class
probabilities will be pretty uniform, so that on average the
$\delta(V_{1}, V_{2})$ will be very small.

{\sc mvdm} differs considerably from Overlap based metrics in its
composition of the nearest neighbor sets. Overlap causes an abundance
of ties in nearest neighbor position. For example, if the nearest
neighbor is at a distance of one mismatch from the test instance, then
the nearest neighbor set will contain the entire partition of the
training set that matches all the other features but contains {\em
any} value for the mismatching feature (see~\namecite{Zavrel+97} for a
more detailed discussion). With the {\sc mvdm} metric, however, the
nearest neighbor set will either contain patterns which have the value
with the lowest $\delta(V_{1}, V_{2})$ in the mismatching position, or
{\sc mvdm} will select a totally different nearest neighbor which has
less exactly matching features, but a smaller distance in the
mismatching features. In sum, this means that the nearest neighbor set
is usually much smaller for {\sc mvdm} at the same value of $k$. In
NLP tasks we have found it very useful to experiment with values of
$k$ larger than one for {\sc mvdm}, because this re-introduces some of
the beneficial smoothing effects associated with large nearest
neighbor sets.

One cautionary note about this metric is connected with data
sparsity. In many practical applications, we are confronted with a
very limited set of examples. This poses a serious problem for the
{\sc mvdm} metric. Many values occur only once in the whole data
set. This means that if two such values occur with the same class the
{\sc mvdm} will regard them as identical, and if they occur with two
different classes their distance will be maximal. The latter condition
reduces the {\sc mvdm} to the Overlap metric for many cases, with the
addition that some cases will be counted as an exact match or mismatch
on the basis of very shaky evidence.

\subsection{Distance Weighted Class Voting}
\label{distweightvote}

The most straightforward method for letting the $k$ nearest neighbors
vote on the class of a new case is the {\em majority voting} method,
in which the vote of each neighbor receives equal weight, and the
class with the highest number of votes is chosen. 

We can see the voting process of the $k$-NN classifier as an attempt
to make an optimal class decision, given an estimate of the
conditional class probabilities in a local region of the data space.
The radius of this region is determined by the distance of the
$k$-furthest neighbor.

Sometimes, if $k$ is small, and the data is very sparse, or the class
labels are noisy, the ``local'' estimate is very unreliable.  As it
turns out in experimental work, using a larger value of $k$ can often
lead to higher accuracy. The reason for this is that in densely
populated regions, with larger $k$ the local estimates become more
reliable, because they are "smoother".  However, when the majority
voting method is used, smoothing can easily become oversmoothing in
sparser regions of the same data set. The reason for this is that the
radius of the $k$-NN region can become extended far beyond the local
neighborhood of the query point, but the far neighbors will receive
equal influence as the close neighbors. This can result in
classification errors that could easily have been avoided if the
measure of influence would somehow be correlated with the measure of
similarity. To remedy this, we have implemented three types of distance 
weighted voting functions in version 4.1 of TiMBL (depicted in 
Figure~\ref{dist-weight-fig}).

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=\columnwidth
                \epsffile{dist-weight.eps}
                \caption{Three functions for determining the weight of the vote
of a nearest neighbor given its distance
from the query. a) Inverse Linear weighting b) Inverse Distance weighting and
c) Exponential Decay weighting.
                }
                \label{dist-weight-fig}
        \end{center}
\end{figure}

A voting rule in which the votes of different members of the nearest
neighbor set are weighted by a function of their distance to the
query, was first proposed by Dudani~\shortcite{Dudani76}. In this scheme, a
neighbor with smaller distance is weighted more heavily than one with
a greater distance: the nearest neighbor gets a weight of 1, the
furthest neighbor a weight of 0 and the other weights are scaled
linearly to the interval in between (\namecite{Dudani76}, equation 1.).

\begin{equation}
\label{dudani_eq}
w_{j}= \left \{ \begin{array}{ll}  
                        \frac{d_{k} - d_{j}}{d_{k} - d_{1}} & \mbox{if $d_{k}
\not= d_{1}$ } \\ 
                        1 & \mbox{if $d_{k} = d_{1}$}\\
                    \end{array} 
          \right. 
\end{equation}

Where $d_{j}$ is the distance to the query of the $j$'th nearest
neighbor, $d_{1}$ the distance of the nearest neighbor, and $d_{k}$ of
the furthest ($k$'th) neighbor.

Dudani (\namecite{Dudani76}, eq.2,3) further proposed the {\em inverse
distance weight}. In equation~\ref{inverseweight} a small constant is
usually added to the denominator to avoid division by
zero~\cite{Wettschereck94}.

\begin{equation}
\label{inverseweight}
w_{j}= \left \{ \begin{array}{ll}  
                        \frac{1}{d_{j}} & \mbox{if $d_{j} \not= 0$ } \\ 
                    \end{array} 
          \right. 
\end{equation}

Another weighting function considered here is based on the work of
\namecite{Shepard87}, who argues for a universal perceptual law
which states that the relevance of a previous stimulus for the
generalization to a new stimulus is an exponentially decreasing
function of its distance in a psychological space. This gives the
weighed voting function of equation~\ref{expdecayweight}, where
$\alpha$ and $\beta$ are constants determining the slope and the power
of the exponential decay function. In the experiments reported below
we have set $\alpha$ and $\beta$ equal to 1.0 unless indicated
otherwise.

\begin{equation}
\label{expdecayweight}
w_{j}= e^{-\alpha d_{j}^\beta}
\end{equation}

Note that in equations~\ref{inverseweight} and ~\ref{expdecayweight}
the weight of the nearest and furthest neighbors and the slope between
them depend on their absolute distance to the query. This assumes that
the relationship between absolute distance and the relevance gradient
is fixed over different datasets.

Following Dudani's proposal, the benefits of weighted voting for
$k$-NN have been discussed widely
(e.g. \namecite{Bailey+78,Morin+81,MacLeod+87}), but mostly
from an analytical perspective. With the popularity of Instance-Based
Learning applications, these issues have gained a more practical
importance. In his thesis on $k$-NN classifiers,
\namecite{Wettschereck94} cites Dudani, but proceeds to work
with equation~\ref{inverseweight}. He tested this function on a large
amount of datasets and found weak evidence for performance increase
over majority voting. An empirical comparison of the discussed
weighted voting methods in~\cite{Zavrel97} has shown that weighted
voting indeed often outperforms unweighted voting, and that Dudani's
original method (Equation~\ref{dudani_eq}) mostly outperforms the
other two methods. From that set of experiments, it also seems that
Dudani's method shows its optimal performance at much larger values of
$k$ than the other voting methods.


\section{Indexing optimizations}
\label{indexing}

The discussion of the algorithm and the metrics in the section above
is based on a naive implementation of nearest neighbor search: a flat
array of instances which is searched from beginning to end while
computing the similarity of the test instance with each training
instance (see the left part of Figure~\ref{example1}). Such an
implementation, unfortunately, reveals the flip side of the lazy
learning coin. Although learning is very cheap: just storing the
instances in memory, the computational price of classification can
become very high for large data sets. The computational cost is
proportional to $N$, the number of instances in the training set.

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=0.8\columnwidth
                \epsffile{example1.eps}
                \caption{The instance base for a small object
                classification toy problem. The left figure shows a
                flat array of instances through which sequential
                nearest neighbor search is performed to find the best 
		match for a test instance (shown below the instance
                base). 
		In the right part, an inverted index (see text)
                is used to restrict the search to those instances
                which share at least one feature value with the test
                instance.
                }
                \label{example1}
        \end{center}
\end{figure}

In our implementation of {\sc ib1} we use two indexing optimizations
that alleviate this problem, (1) tree-based memory and (2) inverted
indices.

\subsection{Tree-based memory}

The tree-based memory indexing operation replaces the flat array
by a tree structure. Instances are stored in the tree as
paths from a root node to a leaf, the arcs of the path are the
consecutive feature-values, and the leaf node contains a {\em
distribution} of classes, i.e.~a count of how many times which class
occurs with this pattern of feature-values (see Figure~\ref{example2}).

Due to this storage structure, instances with identical feature-values
are collapsed into one path, and only their separate class
information needs to be stored in the distribution at the leaf
node. Many different {\bf tokens} of a particular {\bf instance type}
share one path from the root to a leaf node. Moreover, instances which
share a prefix of feature-values, also share a partial path. This
reduces storage space (although at the cost of some book-keeping
overhead) and has two implications for nearest neighbor search
efficiency.

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=0.8\columnwidth
                \epsffile{example2.eps}\
                \caption{A tree-structured storage of the instance
                base from figure~\ref{example1}. An exact match for
                the test is in this case directly found by a top down
                traversal of the tree (grey path). If there is no
                exact match, all paths are interpreted as instances
                and the distances are computed. The order of the
                features in this tree is based on Gain Ratio.
                }
                \label{example2}
        \end{center}
\end{figure}

In the first place, the tree can be searched top-down very quickly for
{\em exact matches}. When $k=1$, an exact match ($\Delta(X,Y)=0$) can
never be beaten, so then it is possible to omit any further distance
computations. TiMBL implements this shortcut. It does not use it with
$k>1$; it does, however, offer the possibility to use the shortcut
anyway (with the command line switch ({\tt +x}). Using it can speed up
classification radically for some types of data, but with $k>1$, the
shortcut is not guaranteed to give the same performance (for better or
for worse) as classification without it.

Second, the distance computation for the nearest neighbor search can
re-use partial results for paths which share prefixes.  This re-use of
partial results is in the direction from the root to the leaves of the
tree. When we have proceeded to a certain level of the tree, we know
how much similarity (equation~\ref{overlapeq}) can still contribute to
the overall distance (equation~\ref{distance}), and discard whole
branches of the tree which will never be able to rise above the
partial similarity of the current least similar best neighbor. By
doing the search depth first\footnote{Suggested by Gert Durieux.}, the
similarity threshold quickly gets initialized to a good value, so that
large parts of the search space can be pruned.

Disregarding this last constraint on search, the number of feature-value
comparisons is equal to the number of arcs in the tree. Thus if we can
find an ordering of the features which produces more overlap between
partial paths, and hence a smaller tree, we can gain both space and
time improvements. An ordering which was found to produce small trees
for many of our NLP data sets is Gain Ratio divided by the number of
feature-values (this is the default setting). Through the {\tt -T}
command line switch, however, the user is allowed to experiment with
different orderings.

\subsection{Inverted indices}

The second method implemented in TiMBL for nearest neighbor search
efficiency is a speedup optimization based on the following fact. Even
in the tree-based structure, the distance is computed between the test
instance and {\em all} instance types. This means that even instance
types which do not share a single feature-value with the test instance
are considered, although they will surely yield a zero similarity. The
use of an {\bf inverted index} excludes these zero similarity
patterns.  The construction of the inverted index records for all
values of each feature a list of instance types (i.e. leaf nodes in
the tree described in the previous section) in which they occur. Thus
it is an inverse of the instance-base, which records for each instance
type which feature-values occur in it\footnote{Unfortunately this also
implies that the storage of both an instance-base and an inverted
index takes about twice the amount of memory. Therefore we have chosen
to turn it off by default. However, especially for data with very
large numbers of low frequent values it can be substantially faster to
turn it on.}.

When a test instance is to be classified, we select the lists of
instance types for the feature-values that it contains (illustrated in
the rightmost part of Figure~\ref{example1}). We can now find the
nearest neighbor in these lists in a time that is proportional to the
number of occurrences of the most frequent feature-value of the test
pattern, instead of proportional to the number of instance types.

Although worst case complexity is still proportional to $N$, the size
of the training set, and practical mileage may vary widely depending
on the peculiarities of your data, the combination of exact match
shortcut, tree-based path re-use, and inverted index has proven in
practice (for our NLP datasets) to make the difference between hours
and seconds of computation\footnote{{\sc mvdm} and numeric features
cannot make use of the inverted index optimization, because it can
happen that two cases with not one value in common are still nearest
neighbors. 
%
%Because the precomputation of differences between
%values is often impossible in tasks with a large number of feature
%values ($n^2$ differences must be stored per feature, if $n$ is the
%number of values of that feature), and because {\sc mvdm} then
%effectively multiplies the number of distance computations per
%instance by the number of classes, this metric is currently one of the
%slowest in the package.
}.

\section{IGTree}
\label{igtree}

Using Information Gain rather than unweighted Overlap distance to
define similarity in {\sc ib1} improves its performance on several
{\sc nlp} tasks \cite{Daelemans+92b,VandenBosch+93,VandenBosch97}.
The positive effect of Information Gain on performance prompted us to
develop an alternative approach in which the instance memory is
restructured in such a way that it contains the same information as
before, but in a compressed decision tree structure. We call this
algorithm {\sc igtree}~\cite{Daelemans+97} (see Figure~\ref{example3}
for an illustration). In this structure, similar to the
tree-structured instance base described above, instances are stored as
paths of connected nodes which contain classification
information. Nodes are connected via arcs denoting feature
values. Information Gain is used to determine the order in which
instance feature-values are added as arcs to the tree. The reasoning
behind this compression is that when the computation of information
gain points to one feature clearly being the most important in
classification, search can be restricted to matching a test instance
to those memory instances that have the same feature-value as the test
instance at that feature. Instead of indexing all memory instances
only once on this feature, the instance memory can then be optimized
further by examining the second most important feature, followed by
the third most important feature, etc.  Again, considerable
compression is obtained as similar instances share partial paths.

\begin{figure}[htb]
        \begin{center}
                \leavevmode
                \epsfxsize=0.7\columnwidth
                \epsffile{example3.eps}
                \caption{A pruned {\sc igtree} for the instance base
                of Figure~\ref{example1}. The classification for
                the test instance is found by top down search of the
                tree, and returning the class label (default) of the
                node after the last matching feature-value (arc). Note
                that this tree is essentially a compressed version of
                the tree in Figure~\ref{example2}.
                }
                \label{example3}
        \end{center}
\end{figure}

Because {\sc igtree} makes a heuristic approximation of nearest
neighbor search by a top down traversal of the tree in the order of
feature relevance, we no longer need to store all the paths. The idea
is that it is not necessary to fully store those feature-values of the
instance that have lower Information Gain than those features which
already fully disambiguate the instance classification.

Apart from compressing all training instances in the tree structure,
the {\sc igtree} algorithm also stores with each non-terminal node
information concerning the {\em most probable} or {\em default}
classification given the path thus far, according to the bookkeeping
information maintained by the tree construction algorithm. This extra
information is essential when processing unknown test instances.
Processing an unknown input involves traversing the tree (i.e.,
matching all feature-values of the test instance with arcs in the
order of the overall feature Information Gain), and either retrieving
a classification when a leaf is reached (i.e., an exact match was
found), or using the default classification on the last matching
non-terminal node if an exact match fails.

In sum, it can be said that in the trade-off between computation
during learning and computation during classification, the {\sc
igtree} approach chooses to invest more time in organizing the
instance base using Information Gain and compression, to obtain
considerably simplified and faster processing during classification,
as compared to {\sc ib1} and {\sc ib1-ig}.
 
The generalization accuracy of {\sc igtree} is usually comparable to
that of {\sc ib1-ig}; most of the time not significantly differing,
and occasionally slightly (but statistically significantly) worse, or
even better.  The two reasons for this surprisingly good accuracy are
that (i) most `unseen' instances contain considerably large parts that
fully match stored parts of training instances, and (ii) the
probabilistic information stored at non-terminal nodes (i.e., the
default classifications) still produces strong `best guesses' when
exact matching fails. The difference between the top-down traversal of
the tree and precise nearest neighbor search becomes more pronounced
when the differences in informativity between features are small. In
such a case a slightly different weighting would have produced a
switch in the ordering and a completely different tree. The result can
be a considerable change in classification outcomes, and hence also in
accuracy. However, we have found in our work on NLP datasets that when
the goal is to obtain a very fast classifier for processing large
amounts of text, the slight tradeoff between accuracy and speed can be
very attractive. Note, also, that by design, {\sc igtree} is not
suited for numeric features, as long as it does not use some type of
discretization. In TiMBL numbers will simply be treated as literal
strings in this case. Moreover, one should realize that the success of
{\sc igtree} is determined by a good judgement of feature relevance
ordering. Hence {\sc igtree} is not to be used with e.g. ``no
weights'' ({\tt -w 0}). Also, setting the {\tt -k} parameter has no
effect on {\sc igtree} performance.

\section{The TRIBL hybrid}
\label{tribl}

The application of {\sc igtree} on a number of common machine-learning
datasets suggested that it is not applicable to problems where the
relevance of the predictive features cannot be ordered in a
straightforward way, e.g.~if the differences in Information Gain are
only very small. In those cases, {\sc ib1-ig} or even {\sc ib1} tend
to perform significantly better than {\sc igtree}.

For this reason we have designed {\sc tribl}, a hybrid generalization
of {\sc igtree} and {\sc ib1}. {\sc tribl} allows you to exploit the
trade-off between (i) optimization of search speed (as in {\sc
igtree}), and (ii) maximal generalization accuracy. To achieve this, a
parameter is set determining the switch from {\sc igtree} to {\sc
ib1}. A heuristic that we have used with some success is based on {\em
average feature information gain}; when the Information Gain of a
feature exceeds the sum of the average Information Gain of all
features $+$ one standard deviation of the average, then the feature
is used for constructing an {\sc igtree}, including the computation of
defaults on nodes. When the Information Gain of a feature is below
this threshold, and the node is still ambiguous, tree construction
halts and the leaf nodes at that point represent case bases containing
subsets of the original training set. During search, the normal {\sc
igtree} search algorithm is used, until the case-base nodes are
reached, in which case regular {\sc ib1} nearest neighbor search is
used on this sub-case-base. In TiMBL, however, you must specify the
switch point from {\sc igtree} to {\sc ib1}, also referred to as
``{\sc tribl} offset'', manually.

\section{IB2: Incremental editing}
\label{ib2}

In memory-based learning it seems sensible to keep any instance in
memory that plays a (potentially) positive role in the correct
classification of other instances. Alternatively, when it plays no
role at all, or when it is disruptive for classification, it may be a
good idea to discard, or {\em edit} it from memory. On top of not
harming or even improving generalization performance, the editing of
instances from memory could also alleviate the practical processing
burden of the $k$-NN classifier kernel, since it would have less
instances to compare new instances to. This potential double pay-off
spawned a distinct line of work on editing in the $k$-NN classifier
quite early \namecite{Hart68} and \namecite{Wilson72}
(for overviews, cf. \namecite{Dasarathy91,VandenBosch99}).

TiMBL offers an implementation of one particular editing algorithm
called {\sc ib2} \cite{Aha+91}, an extension to the basic {\sc ib1}
algorithm introduced in the same article. {\sc ib2} implements an
incremental editing strategy. Starting from a seed memory filled with
a certain (usually small) number of labeled training instances, {\sc
ib2} adds instances incrementally to memory only when they are {\em
misclassified}\/ by the $k$-NN classifier on the basis of the
instances in memory at that point. These instances are added, since
they are assumed to be representatives of a part of the complete
instance space in which they themselves and potentially more
nearest-neighbor instances have a particular class different from the
class of neigboring instances already in memory. The economical idea
behind {\sc ib2} is that this way tupically only instances on the
boundaries of such areas are stored, and not the insides of the areas;
the classification of instances that would be positioned well inside
such areas is assumed to be safeguarded by the memorized boundary
instances surrounding it.

Although the {\sc ib2} may optimize storage
considerably, its strategy to store all misclassified instances
incrementally makes {\sc ib2} sensitive to noise \cite{Aha+91}. It is
also yet unclear what the effect is of the size of the seed.

\section{NLP applications of TiMBL}
\label{furtherreading}

This section provides a brief historical overview of work, performed
in the Tilburg and Antwerp groups and by others, with the application
of {\sc mbl}-type algorithms to NLP tasks.

The Tilburg and Antwerp groups have published a number of articles
containing descriptions of the algorithms and specialised metrics
collected in TiMBL, usually demonstrating their functioning using NLP
tasks. The {\sc ib1-ig} algorithm was first introduced in
\cite{Daelemans+92b} in the context of a comparison of memory-based
approaches with error-back\-propagation learning for a hyphenation
task.  Predecessor versions of {\sc igtree} can be found in
\cite{Daelemans+93c,VandenBosch+93} where they are applied to
grapheme-to-phoneme conversion.  See \cite{Daelemans+97} for a
description and review of {\sc igtree} and {\sc ib1-ig}. {\sc tribl}
is described in \cite{Daelemans+97d}.  Experiments with
distance-weighted class voting are described in
\cite{Zavrel97}. Aspects of using binary-valued (unpacked
multi-valued) features are discussed in \cite{VandenBosch+00}.
Comparisons between memory-based learning and editing variants are
reported in \cite{VandenBosch99,Daelemans+99}. A hybrid of TiMBL and
the {\sc ripper} rule-induction algorithm \cite{Cohen95} is described
in \cite{VandenBosch00}. Using TiMBL as a classifier combination
method is discussed in \cite{Halteren+01}. \namecite{Raaijmakers00}
describes an extension of TiMBL with error-correcting output codes.

The memory-based algorithms implemented in the TiMBL package have been
targeted to a large range of Natural Language Processing
tasks. Examples of applications in the morpho-phonological and speech areas are
hyphenation and syllabification \cite{Daelemans+92b}; classifiying
phonemes in speech \cite{Kocsor+00}; assignment of word stress
\cite{Daelemans+94b}; grapheme-to-phoneme conversion,
\cite{VandenBosch+93,Daelemans+96}; predicting linking morphemes in
Dutch compounds \cite{Krott+01}; diminutive formation
\cite{Daelemans+98a}; and morphological analysis
\cite{VandenBosch+96,VandenBosch+99}.

Work on syntacto-semantic tasks at the sentence level has  focused on
part of speech tagging \cite{Daelemans+96b,Zavrel+99,Halteren+01};
PP-attachment \cite{Zavrel+97b}; word sense disambiguation
\cite{Veenstra+00,Stevenson+99,Kokkinakis00}; subcategorization
\cite{Buchholz98}; phrase chunking \cite{Veenstra98,Sang+99}; article
generation \cite{Minnen+00}; shallow parsing
\cite{Daelemans+99a,Buchholz+99}; named-entity recognition
\cite{Buchholz+00}; clause identification \cite{Orasan00,Sang01};
sentence-boundary detection \cite{Stevenson+00}; and predicting the
order of prenominal adjectives for generation \cite{Malouf00}.

On the textual level, TiMBL has been used for information extraction
\cite{Zavrel+00b} and spam filtering \cite{Androutsopoulos+00}. In the
field of discourse and dialogue modeling, TiMBL has been used for
shallow semantic analysis of speech-recognised utterances
\cite{Gustafson+99} and in error detection in spoken dialogue systems
\cite{Krahmer+01,VandenBosch+01}.

Relations to statistical language processing are discussed in
\cite{Zavrel+97}. The first
dissertation-length study devoted to the approach is
\cite{VandenBosch97}, in which the approach is compared to alternative
learning methods for NLP tasks related to English word pronunciation
(stress assignment, syllabification, morphological analysis,
alignment, grapheme-to-phoneme conversion). In 1999 a special issue of
the {\em Journal for Experimental and Theoretical Artificial
Intelligence} (Vol.~11(3), edited by Walter Daelemans) was devoted to
Memory-Based Language Processing. The introduction to this special
issue discusses the inspiration sources and alternative developments
related to the memory-based approach taken in TiMBL
\cite{Daelemans99b}.

Whereas most work using TiMBL has been oriented towards {\em language
engineering} applications, the linguistic and psycholinguistic
relevance of memory-based learning is another focus of the research in
Antwerp and Tilburg. Work in this area has been done on stress
assignment in Dutch \cite{Daelemans+94b,Gillis+00}, reading aloud
\cite{Vandenbosch+00b}, phonological bootstrapping \cite{Durieux+00},
the above-mentioned prediction of linking morphemes in Dutch
\cite{Krott+01}, and comparison to other analogical methods for
linguistics \cite{Daelemans+97f}.

\ \\

{\it All Tilburg/Antwerp papers referred to in this section and more recent
material are available in electronic form from the {\sc ILK} homepage:
{\tt http://ilk.kub.nl} or the {\sc CNTS} homepage: \\ {\tt
http://cnts.uia.ac.be}. We are grateful for any feedback on the
algorithms and the way we applied them.}

\chapter{File formats}
\label{fileformats}

This chapter describes the format of the input and output files used
by TiMBL. Where possible, the format is illustrated using the same
small toy data set that is shown in Figure~\ref{example1}. It consists
of 12 instances of 5 different everyday objects (nut, screw, key, pen,
scissors), described by 3 discrete features (size, shape, and number
of holes).

\section{Data format}
\label{dataformats}

The training and test sets for the learner consist of descriptions of
instances in terms of a fixed number of feature-values. TiMBL supports
a number of different formats for the instances, but they all have in
common that the files should contain one instance per line. The number
of instances is determined automatically, and the format of each
instance is inferred from the format of the first line in the training
set. The last feature of the instance is assumed to be the target
category. Should the guess of the format by TiMBL turn out to be
wrong, you can force it to interpret the data as a particular format
by using the {\tt -F} option. Note that TiMBL, by default, will
interpret features as having {\em symbolic, discrete values}. Unless
you specify explicitly that certain features are numeric, using the
{\tt -M} option, TiMBL will interpret numbers as just another string
of characters. If a feature is numeric, its values will be scaled to
the interval [0,1] for purposes of distance computation (see
Equation~\ref{overlapeq}). The computation of feature weights will be
based on a discretization of the feature.

Once TiMBL has determined the input format, it will skip and complain
about all lines in the input which do not respect this format
(e.g.~have a different number of feature-values with respect to that
format).

During testing, TiMBL writes the classifications of the test set to an
output file. The format of this output file is by default the same as
the input format, with the addition of the predicted category being
appended after the correct category. If we turn on higher levels of
verbosity, the output files will also contain distributions, distances
and nearest neighbor sets.

\subsection{Column format}
\label{comlumnformat}

The {\bf column format} uses white space as the separator between
features. White space is defined as a sequence of one or more spaces or
tab characters. Every instance of white space is interpreted as a
feature separator, so it is not possible to have feature-values
containing white space. The column format is auto-detected when an
instance of white space is detected on the first line {\em before a
comma has been encountered}. The example data set looks like this in
the column format:

\begin{footnotesize}
\begin{verbatim}
small	compact	1	nut
small	long	none	screw
small	long	1	key
small	compact	1	nut
large	long	1	key
small	compact	none	screw
small	compact	1	nut
large	long	none	pen
large	long	2	scissors
large	long	1	pen
large	other	2	scissors
small	other	2	key
\end{verbatim}
\end{footnotesize}

\subsection{C4.5 format}
\label{c45format}

This format is a derivative of the format that is used by the
well-known C4.5 decision tree learning program~\cite{Quinlan93}.  The
separator between the features is a comma, and the category (viz.  the
last feature on the line) is followed by a period (although this is
not mandatory: TiMBL is robust to missing periods)\footnote{The
periods after the category are not reproduced in the output}.  White
space within the line is taken literally, so the pattern {\tt a, b
c,d} will be interpreted as {\tt `a',` b c',`d'}.  When using this
format, especially with linguistic data sets or with data sets
containing floating point numbers, one should take special care that
commas do not occur in the feature-values and that periods do not
occur within the category.  Note that TiMBL's C4.5 format does not
require a so called {\em namesfile}.  However, TiMBL can produce such
a file for C4.5 with the {\tt -n} option.  The C4.5 format is
auto-detected when a comma is detected on the first line {\em before
any white space has been encountered}. The example data set looks like
this in the C4.5 format:

\begin{footnotesize}
\begin{verbatim}
small,compact,1,nut.
small,long,none,screw.
small,long,1,key.
small,compact,1,nut.
large,long,1,key.
small,compact,none,screw.
small,compact,1,nut.
large,long,none,pen.
large,long,2,scissors.
large,long,1,pen.
large,other,2,scissors.
small,other,2,key.
\end{verbatim}
\end{footnotesize}

\subsection{ARFF format}
\label{arffformat}

ARFF is a format that is used by the WEKA machine learning
workbench~\cite{Garner95}\footnote{WEKA is available from the Waikato
University Department of Computer Science, {\tt
http://www.cs.waikato.ac.nz/\~{}ml/}.}.  Although TiMBL at present
does {\em not} entirely follow the ARFF specification, it still tries to do
as well as it can in reading this format. In ARFF the actual data are
preceded by a header with various types of information and
interspersed with lines of comments (starting with \%). The ARFF
format is auto-detected when the first line starts with \% or @. TiMBL
ignores lines with ARFF comments and instructions, and starts reading
data from after the {\tt @data} statement until the end of the
file. The feature-values are separated by commas, and white space is
deleted entirely, so the pattern {\tt a, b c,d} will be interpreted as
{\tt `a',`bc',`d'}. We hope include better support for the
ARFF format in future releases.

\begin{footnotesize}
\begin{verbatim}
% There are 4 attributes.
% There are 12 instances.
% Attribute information:                       Ints Reals  Enum  Miss
%          'size'                                 0     0    12     0   
%          'shape'                                0     0    12     0   
%          'n_holes'                              9     0     3     0   
%          'class.'                               0     0    12     0   
@relation 'example.data'
@attribute 'size' { small, large}
@attribute 'shape' { compact, long, other}
@attribute 'n_holes' { 1, none, 2}
@attribute 'class.' { nut., screw., key., pen., scissors.}
@data
small,compact,1,nut.
small,long,none,screw.
small,long,1,key.
small,compact,1,nut.
large,long,1,key.
small,compact,none,screw.
small,compact,1,nut.
large,long,none,pen.
large,long,2,scissors.
large,long,1,pen.
large,other,2,scissors.
small,other,2,key.
\end{verbatim}
\end{footnotesize}

\subsection{Compact format}
\label{compactformat}

The compact format is especially useful when dealing with very large
data files. Because this format does not use any feature separators,
file-size is reduced considerably in some cases. The price of this is
that all features and class labels must be of equal length (in
characters) and TiMBL needs to know beforehand what this length
is. You must tell TiMBL by using the {\tt -l} option. The compact
format is auto-detected when neither of the other formats applies. The
same example data set might look like this in the column format (with
two characters per feature):

\begin{footnotesize}
\begin{verbatim}
smco1_nu
smlonosc
smlo1_ke
smco1_nu
lalo1_ke
smconosc
smco1_nu
lalonope
lalo2_sc
lalo1_pe
laot2_sc
smot2_ke
\end{verbatim}
\end{footnotesize}

\subsection{Sparse Binary format}
\label{binaryformat}

The sparse binary format is especially useful when dealing with large
numbers of two-valued (binary) features, of which each case only has a
very few active ones, such as e.g. in text categorization. Thus
instead of representing a case as:

\begin{footnotesize}
\begin{verbatim}
1,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,small.
\end{verbatim}
\end{footnotesize}

We can represent it as:

\begin{footnotesize}
\begin{verbatim}
1,8,11,12,18,small.
\end{verbatim}
\end{footnotesize}

This format allows one to specify only the index numbers of the active
features (indexes start at one), while implicitly assuming that
the value for all the remaining features is zero. Because each case
has a different number of active features, we must specify in some
other way what the actual number of features is. This must be done
using the {\tt -N} option.  As the format is very similar to numeric
features, there is no auto-detection for it. It must always be
``forced'', using {\tt -F Binary}. The last feature of a line is
always interpreted as being the category string. A case with only
zeroes can be represented as either `{\tt ``class''} or {\tt ``,class''}.

\section{Weight files}
\label{weightformat}

The feature weights that are used for computing similarities and for
the internal organization of the memory-base can be saved to a file.
A file with weights can be constructed or altered manually and then
read back into TiMBL. The format for the weights file is as follows.
The weights file may contain comments on lines that start with a \#
character. The other lines contain the number of the feature followed
by its numeric weight. An example of such a file is provided
below. The numbering of the weights starts with 1 and follows the same
order as in the data file. If features are to be ignored it is
advisable not to set them to zero, but give them the value ``{\tt
Ignored}'' or to use the {\tt -s} option.

\begin{footnotesize}
\begin{verbatim}
# DB Entropy: 2.29248
# Classes: 5
# Lines of data: 12
# Fea.  Weight
1       0.765709
2       0.614222
3       0.73584
\end{verbatim}
\end{footnotesize}

\section{Value difference files}
\label{mvdmformat}

Using the {\sc mvdm} metric, it can sometimes be interesting to
inspect the matrix of conditional class probabilities from
Equation~\ref{MVDMeq}. By using the {\tt -U} option, we can write the
computed matrix to a file. This way we can see which values are
considered to be similar by the metric. For each feature a row vector
is given for each value, of the conditional probabilities of all the
classes (columns) given that value.

\begin{footnotesize}
\begin{verbatim}
targets A,      B,      C,      D,      E.

feature # 1 Matrix: 
small   0.429   0.286   0.286   0.000   0.000
large   0.000   0.000   0.200   0.400   0.400
 
feature # 2 Matrix: 
compact 0.750   0.250   0.000   0.000   0.000
long    0.000   0.167   0.333   0.333   0.167
other   0.000   0.000   0.500   0.000   0.500
 
feature # 3 Matrix: 
1       0.500   0.000   0.333   0.167   0.000
none    0.000   0.667   0.000   0.333   0.000
2       0.000   0.000   0.333   0.000   0.667
\end{verbatim}
\end{footnotesize}

As long as this format is observed, the file can be modified (manually
or by substituting some other vector-based representations for the
values), and the new matrix can be read in and used with the {\sc
mvdm} metric.

\section{Tree files}
\label{treeformat}

Although the learning phase in TiMBL is relatively fast, it can be
useful to store the internal representation of the data set both for
later usage and for faster subsequent retrieval. In TiMBL, the data
set is stored internally in a tree structure (see
Section~\ref{indexing}). When using {\sc ib1}, this tree
representation contains all the training cases as full paths in the
tree. When using {\sc igtree}, unambiguous paths in the tree are
pruned before it is used for classification or written to file. In
either tree, the arcs represent feature-values and nodes contain class
(frequency distribution) information. The features are in the same
order throughout the tree. This order is either determined by
memory-size considerations in {\sc ib1}, or by feature relevance in
{\sc igtree}. It can explicitly be manipulated using the {\tt -T}
option.

We strongly advise to refrain from manually editing the tree
file. However, the syntax of the tree file is as
follows. After a header consisting of information about the status of
the tree, the feature-ordering (the permutation from the order in the
data file to the order in the tree), and the presence of numeric
features\footnote{Although in this header each line starts with '\#',
these lines cannot be seen as comment lines.} a legenda is given of
numeric hash codes for the class names (one unique integer per class)
and feature value names (one unique integer per value).  Subsequently,
the tree's nodes and arcs are given in a proprietary non-indented
bracket notation.

Starting from the root node, each node is denoted by an opening
parenthesis ``('', followed by an integer coding the default
class. After this, there is the class distribution list, within curly
braces ``\{ \}'', containing a non-empty list of category codes
followed by integer counts. After this comes an optional
comma-separated list of arcs to child nodes, within ``[ ]''
brackets. An arc is labeled with a coded feature value. The node that
the arc leads to again has a class distribution, and any number of
child nodes pointed to by arcs.

The {\sc ib1} tree that was constructed from our example data set looks as
follows:

\begin{footnotesize}
\begin{verbatim}
# Status: complete
# Permutation: < 1, 3, 2 >
# Numeric: .
# Version 4 (Hashed)
#
Classes
1       nut
2       screw
3       key
4       pen
5       scissors
Features
1       small
2       compact
3       1
4       long
5       none
6       large
7       2
8       other

(1{ 1 3, 2 2, 3 3, 4 2, 5 2 }[1(1[3(1[2(1{ 1 3 })
,4(3{ 3 1 })
]
)
,5(2[2(2{ 2 1 })
,4(2{ 2 1 })
]
)
,7(3[8(3{ 3 1 })
]
)
]
)
,6(4[3(3[4(3{ 3 1, 4 1 })
]
)
,5(4[4(4{ 4 1 })
]
)
,7(5[4(5{ 5 1 })
,8(5{ 5 1 })
]
)
]
)
]
)

\end{verbatim}
\end{footnotesize}

The corresponding compressed {\sc igtree} version is considerably smaller. 

\begin{footnotesize}
\begin{verbatim}
# Status: pruned
# Permutation: < 1, 3, 2 >
# Numeric: .
# Version 4 (Hashed)
#
Classes
1       nut
2       screw
3       key
4       pen
5       scissors
Features
1       small
2       compact
3       1
4       long
5       none
6       large
7       2
8       other

(1{ 1 3, 2 2, 3 3, 4 2, 5 2 }[1(1{ 1 3, 2 2, 3 2 }[3(1{ 1 3, 3 1 }[4(3{ 3 1 })
]
)
,5(2{ 2 2 })
,7(3{ 3 1 })
]
)
,6(4{ 3 1, 4 2, 5 2 }[3(3{ 3 1, 4 1 })
,7(5{ 5 2 })
]
)
]
)

\end{verbatim}
\end{footnotesize}

TiMBL tree files generated by versions 1.0 to 3.0 of TiMBL, which do
not contain hashed class and value names, will be recognized as such,
and will be read by TiMBL. Although backward compatibility has been
strived for, it is advisable to regenerate tree files with the current
version of TiMBL, especially in the case of version-1.0 trees. It is
foreseen that in future releases of TiMBL the tree format syntax will
change again, while TiMBL will be continuously able to read trees
generated by older versions.

\chapter{Server interface}
\label{serverformat}

It is not always practical or possible to have all test items in one
static test file. Example cases include:

\begin{itemize}
\item The output of one classifier is being reused as a feature of
some other classifier instantly, hence it is not available until it is
processed.
\item The test items come in at arbitrary time intervals.
\end{itemize}

In such cases it is more practical to load the training patterns once
and have TiMBL stand by as a server waiting for new test items to be
processed. This can be achieved by starting TiMBL with the {\tt -S
portnumber} option. TiMBL will load the training set and do the
necessary preparation of statistics and metrics, and then enter an
infinite loop, waiting for input on the specified portnumber. When a
client connects on this portnumber, the server starts a separate
thread for it to process (classification) commands. A sample client
program is included in the distribution. The client must communicate
with the server using the protocol described below. After accepting
the connection, the server first sends a welcome message to the client:

{\tt Welcome to the Timbl server.}

After this, the server waits for client-side requests.
The client can now issue four types of commands: classify, set
(options), query (status), and exit. The type of command is specified
by the the first string of the request line, which can be abbreviated
to any prefix of the command, up to one letter (i.e.~c,s,q,e). The
command is followed by whitespace and the remainder of the command as
described below.
\begin{description}
\item {\tt classify testcase}\\
      testcase is a pattern of features (must have the same number of
      features as the training set) followed by a category
      string. E.g.: {\tt small,long,1,??.}\\
      Depending on the current settings of the server, it will either
      return the answer 
      \begin{verbatim}
	ERROR { explanation }
      \end{verbatim}
      if something's gone wrong, or the answer
      \begin{verbatim}
CATEGORY {category} DISTRIBUTION { category 1 } DISTANCE { 1.000000} NEIGHBORS
ENDNEIGHBORS
      \end{verbatim}
      where the presence of the {\tt DISTRIBUTION}, {\tt DISTANCE} and {\tt NEIGHBORS}
parts depends upon the current verbosity setting. Note that if the
last string on the answer line is {\tt NEIGHBORS}, the server will
proceed to return lines of nearest neighbor information until the
keyword {\tt ENDNEIGHBORS}.
\item {\tt set option}\\
      where option is specified as a string of commandline options
(described in detail in Section~\ref{commandline} below). Only the
following commandline options are valid in this context: {\tt k, m, Q,
R, w, v, x, -}. The
setting of an option in this client 
does not affect the behavior of the server
towards other clients. The server replies either with {\tt OK} or with
{\tt ERROR \{explanation\}}. 
\item {\tt query}\\
      queries the server for a list of current settings. Returns a
number of lines with status information, starting with a line that
says {\tt STATUS}, and ending with a line that says {\tt ENDSTATUS}. For example:

\begin{footnotesize}
\begin{verbatim}
STATUS
FLENGTH              : 0
MAXBESTS             : 500
NULL_VALUE           : 
TREE_ORDER           : G/V
DECAY                : Z
INPUTFORMAT          : Column
SEED                 : -1
DECAYPARAM           : 1.000000
SAMPLE_WEIGHTS       : -
IGNORE_SAMPLES       : +
PROBALISTIC          : -
VERBOSITY            : F
EXACT_MATCH          : -
USE_INVERTED         : -
GLOBAL_METRIC        : Overlap
METRICS              : 
NEIGHBORS            : 1
PROGRESS             : 100000
TRIBL_OFFSET         : 0
IB2_OFFSET           : 0
WEIGHTING            : GRW
ENDSTATUS
\end{verbatim}
\end{footnotesize}

\item {\tt exit}\\
      closes the connection between this client and the server.
\end{description}

\chapter{Command line options}
\label{commandline}

The user interacts with TiMBL through the use of command line arguments.
When you have installed TiMBL successfully, and you type {\tt Timbl} at the
command line without any further arguments, it will print an overview
of the most basic command line options. 

\begin{verbatim}
TiMBL 4.1 (c) ILK 1998, 1999, 2000, 2001.
Tilburg Memory Based Learner
Induction of Linguistic Knowledge Research Group
Tilburg University / University of Antwerp
Tue Jul 31 11:37:08 2001

usage:  Timbl -f data-file {-t test-file}
or see: Timbl -h
        for all possible options
\end{verbatim}

If you are satisfied with all of the default settings, you can proceed
with just these basics:

\begin{description}

\item {\tt -f <datafile>} : supplies the name of the file with the
training items.
\item {\tt -t <testfile>} : supplies the name of the file with the
test items.
\item {\tt -h} : prints a glossary of all available command line 
options.

\end{description}

The presence of a training file will make TiMBL pass through the first
two phases of its cycle. In the first phase it examines the contents
of the training file, and computes a number of statistics on it
(feature weights etc.). In the second phase the instances from the
training file are stored in memory. If no test file is specified, the
program exits, possibly writing some of the results of learning to
files (see below). If there is a test file, the selected classifier,
trained on the present training data, is applied to it, and the
results are written to a file of which name is a combination of the
name of the test file and a code representing the chosen algorithm
settings. TiMBL then reports the percentage of correctly classified
test items. The default settings for the classification phase are: a
Memory-Based Learner, with Gain Ratio feature weighting, with $k=1$,
and with optimizations for speedy search. If you need to change the
settings, because you want to use a different type of classifier, or
because you need to make a trade-off between speed and memory-use,
then you can use the options that are shown using {\tt -h}. The
sections below provide a reference to the use of these command line
arguments, and they are roughly ordered by the type of action that the
option has effect on. Note that some options (listed with ``{\tt
+/-}'') can be turned on ({\tt +}) or off ({\tt -}).

\section{Algorithm and Metric selection}

\begin{description}

\item {\tt -a <string>} : determines the classification algorithm. Possible
values are:

	\begin{description}
	\item n=IB1 -- the {\sc ib1} ($k$-NN) algorithm (default). See Sections~\ref{mbl} and~\ref{indexing}.
	\item n=IGTREE -- {\sc igtree}, decision-tree-based optimization. See Section~\ref{igtree}.
	\item n=TRIBL -- {\sc tribl}, the hybrid of {\sc ib1} and {\sc igtree}. See Section~\ref{tribl}.
	\item n=IB2 -- {\sc ib2}, incremental edited memory-based learning. See Section~\ref{ib2}.
	\end{description}

The old syntax of {\tt -a <n>} still applies, where n=0 means IB1, n=1
means IGTREE, n=2 means TRIBL, and n=3 means IB2.

\item {\tt -m <string>} : determines which similarity metrics are used
for each feature. The format of this string is as follows:\\ {\tt
GlobalMetric:MetricRange:MetricRange}\\ Where {\tt GlobalMetric} is
used for alle features except for the ones that are assigned other
metrics by following the restrictions given by {\tt :MetricRange}. The
metric code can be one of O, M, N, and I, which stand for Overlap
(default), Modified value difference ({\sc mvdm}), Numeric, or Ignore,
respectively. Ignore cannot be the global metric. A range
can be written using comma's for lists, and hyphens for intervals.

For example, {\tt -mO:N3:I2,5-7} sets the global metric to overlap,
declares the third feature to be numeric, and ignores features 2 and
5, 6, and 7.

\item {\tt -w <n>} : chooses between feature-weighting possibilities.
The weights are used in the metric of {\sc ib1} and in the ordering of the
{\sc igtree}. Possible values are:

	\begin{description}
	\item n=0 -- No weighting, i.e. all features have the same
	importance (weight = 1).
	\item n=1 -- Gain Ratio weighting (default). See section~\ref{infogain}.
	\item n=2 -- Information Gain weighting. See section~\ref{infogain}.
	\item n=3 -- Chi-squared ($\chi^2$) weighting. See section~\ref{chisquared}.
	\item n=4 -- Shared variance weighting. See section~\ref{chisquared}.
	\item n=filename -- Instead of a number we can supply a
	filename to the {\tt -w} option. This causes TiMBL to read this file
	and use its contents as weights. (See section~\ref{weightformat} for a
	description of the weights file)
	\end{description}

\item {\tt -b <n>} : determines n ($\geq 1$), the number of instances,
to be taken from the top of the training file, to act as the bootstrap
set of memorized instances before {\sc ib2} starts adding new
instances. Only applicable in conjunction with {\sc ib2} ({\tt -a 3}).

\item {\tt -d <val>} : The type of class voting weights that are used for
extrapolation from the nearest neighbor set. {\tt val} can be one of:
	\begin{itemize} 
  	\item {\tt Z} : normal majority voting; all neighbors have
         equal weight (default).
  	\item {\tt ID}: Inverse Distance weighting. See Section~\ref{distweightvote}, Equation~\ref{dudani_eq}.
  	\item {\tt IL}: Inverse Linear weighting. See Section~\ref{distweightvote}, Equation~\ref{inverseweight}.
  	\item {\tt ED<a>}: Exponential Decay weighting with decay parameter {\tt a}. No space is allowed between {\tt ED} and {\tt a}. See Section~\ref{distweightvote}, Equation~\ref{expdecayweight}.
\end{itemize}

\item {\tt -k <n>} : Number of nearest neighbors used for
extrapolation. Only applicable in conjunction with {\sc ib1} ({\tt -a
0}), {\sc tribl} ({\tt -a 2}), and {\sc ib2} ({\tt -a 3}). The default
is 1. Especially with the {\sc mvdm} metric it is often useful to
determine a good value larger than 1 for this parameter (usually an
odd number, to avoid ties). Note that due to ties (instances with
exactly the same similarity to the test instance) the number of
instances used to extrapolate might in fact be much larger than this
parameter.

\item {\tt -q <n>} : {\tt n} is the {\sc tribl} offset, the index
number of the feature where {\sc tribl} should switch from {\sc
igtree} to {\sc ib1}. Only applicable in conjunction with {\sc ib2}
({\tt -a 3}).

\item {\tt -R <n>} : Resolve ties in the classifier randomly, using a
random generator with seed n. As a default this is OFF, and ties are
resolved in favor of the category which is more frequent in the
training set as a whole---remaining ties are resolved on a first come
first served basis. For comparison purposes, we have also included the
option {\tt -R <n>}, which causes the classification to be based on
a random pick (with seed n) of a category according to the probability
distribution in the nearest neighbor set.

\item {\tt -t <@file>} : If the filename given after {\tt -t} starts
with '{\tt @}', TiMBL will read commands for testing from {\tt file}.
This file should contain one set of instructions per line. On each
line new values can be set for the following command line options:
{\tt -e -F -k -m -o -p -Q -R -t -u +/-v -w +/-x +/-\% +/--}. It is compulsory
that each line in {\tt file} contains a {\tt -t <testfile>} argument
to specify the name of the test file.

\item {\tt -t <testfile>} : the string {\tt <testfile>} is the literal name
of the file with the test items.

\item {\tt -t leave\_one\_out} : No test file is read, but testing is
done on each pattern of the training file, by treating each pattern of
the training file in turn as a test case (and the whole remainder of
the file as training cases).

\item {\tt -t cross\_validate} : An $n$-fold cross-validation
experiment is performed on the basis of $n$ files (e.g. $1/n$
partitionings of an original data file). The names of these $n$ files
need to be in a text file (one name per line) which is given as
argument of {\tt -f}. In each fold $f=1 \ldots n$, file number $f$ is
taken as test set, and the remaining $n-1$ files are concatenated to
form the training set.

\end{description}

\section{Input options}

\begin{description}

\item {\tt -f <datafile>} : the string {\tt <datafile>} is the literal name
of the file with the training items.

\item {\tt -F <format>} : Force TiMBL to interpret the training and
test file as a specific data format. Possible values for this
parameter are: {\tt Compact, C4.5, ARFF, Columns, Binary}
(case-insensitive). The default is that TiMBL guesses the format from
the contents of the first line of the data file. See
section~\ref{dataformats} for description of the data formats and the
guessing rules. The {\tt Compact} format cannot be used with numeric
features.

\item {\tt -l <n>} : Feature length. Only applicable with the Compact
data format; {\tt <n>} is the number of characters used for each
feature-value and category symbol.

\item {\tt -i <treefile>} : Skip the first two training phases, and
instead of processing a training file, read a previously saved (see
{\tt -I} option) instance-base or {\sc igtree} from the file {\tt
treefile}. See section~\ref{treeformat} for the format of this file.

\item {\tt -u <valueclassprobfile>} : Replace the automatically computed
value-class probability matrix with the matrices provided in this
file.

\item {\tt -P <path>} : Specify a path to read the data files
from. This path is ignored if the name of the data file already
contains path information.

\item {\tt -s}: Use the whitespace-delimited examplar weights, given
after each training instance in the training file {\tt <datafile>},
during classification.

\item {\tt -S <portnumber>} : Starts a TiMBL server listening on the
specified port number of the localhost. See
Chapter~\ref{serverformat} for a description of the communication
protocol.

\end{description}

\section{Output options}

\begin{description}

\item {\tt -I <treefile>} : After phase one and two of learning, save
the resulting tree-based representation of the instance-base or {\sc
igtree} in a file. This file can later be read back in using the {\tt
-i} option (see above). See section~\ref{treeformat} for a description
of the resulting file's format. This also automatically saves the
current weights into {\tt treefile.wgt} unless this is overridden by
{\-W}.

\item {\tt -W <file>} : Save the currently used feature-weights in a
file.

\item {\tt -U <valueclassprobfile>} : Write the automatically computed
value-class probability matrix to this file.

\item {\tt -n <file>} : Save the feature-value and target category
symbols in a C4.5 style ``names file'' with the name {\tt
<file>}. Take caution of the fact that TiMBL does not mind creating a
file with ',' '.' '|' and ':' values in features. C4.5 will choke on this.

\item {\tt -p <n>} : Indicate progress during training and testing
after every n processed patterns. The default setting is 10000.

\item {\tt -e <n>} : During testing, compute and print an estimate on
how long it will take to classify n test patterns. This is off by
default.

\item {\tt -V} : Show the version number.

\item {\tt +/-v <n>} : Verbosity Level; determines how much
information is written to the output during a run. Unless indicated
otherwise, this information is written to standard error. The use of
{\tt +} turns a given verbosity level {\bf on}, whereas {\tt -} turns
it {\bf off} (only useable in non-commandline contexts, such as
client/server communication or {\tt -t @testcommandfile}). This
parameter can take on the following values:

	\begin{description}
         \item s:  work silently (turns off all set verbosity levels).
         \item o:  show all options set.
         \item f:  show Calculated Feature Weights. (default)
         \item p:  show {\sc mvdm} matrices.
         \item e:  show exact matches.
	 \item cm: show confusion matrix between actual and predicted classes.
         \item di: add distance to output file.
         \item db: add distribution of best matched to output file
         \item n:  add nearest neigbors to output file (sets {\tt -x}
         and {\tt --})
	\item You may combine levels using '{\tt +}' e.g. {\tt +v
         p+db} or {\tt -v o+di}.
	\end{description}

\item {\tt +/- \%} : Write the percentage of correctly classified test
                 instances to a file with the same name as the output
                 file, but with the suffix ``{\tt .\%}''.

\item {\tt -o <filename>} : Write the test output to filename. Useful
for different runs with the same settings on the same testfile.

\item {\tt -O <path>} : Write all output to the path given here. The
default is to write all output to the directory where the test file
is located.

\end{description}

\section{Internal representation options}

\begin{description}

\item {\tt -N <n>} : (maximum) number of features. Obligatory for
Binary format. When larger than a pre-defined constant (default 2500),
N needs to be set explicitly for all algorithms.

\item {\tt -M <string>} : use {\tt string} as missing value. Missing
values have an effect with {\sc mvdm} ({\tt -mM}): when {\tt
string} matches with {\tt string}, their distance is 0.0; when {\tt
string} matches with another value, their distance is 2.0 (the maximum
for {\sc mvdm}). With {\tt -mO}, missing values behave like ordinary
symbols. Missing values also have an effect on Overlap ({\tt -mO})
because they are not taken into account when computing feature
weights. 

\item {\tt +/- x} : turns the shortcut search for exact matches on or
off in {\sc ib1} (and {\sc ib2} and {\sc tribl}). The default is to be
off ({\tt -x}). Turning it on makes {\sc ib1} generally faster, but
with $k>1$ the shortcut produces different results from a ``real'' $k$
nearest neighbors search, since absolute preference is given to the
exact match.

\item {\tt +/- -} : Turn on/off the use of inverted files. Turning
this on will sometimes make testing (considerably) faster, but
approximately doubles the memory load. The default is off, but it is
set on automatically with {\tt -F Binary} files.

\item {\tt -B <n>}: Set the maximum number of nearest neighbors
printed using the {\tt +vn} verbosity option. By default this is set
to 500, but when you are interested in the contents of really large
nearest neighbor sets (which is possible with large $k$ or large data
sets with few features), {\tt n} can be increased up to 10,000.

\end{description}

\clearpage

\bibliographystyle{fullname}
\bibliography{timblman}

\end{document}
